{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function derivative interactive notebook\n",
    "This notebook has been designed to demonstrate how the derivative of a function can be ploted in order to show how the slope behave in relation to the increasing or decreasing values of a function.\n",
    "This information is especially useful when one is learning how neural networks works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Enables Operational System integration\n",
    "import math # Some math stuff\n",
    "import numpy as np # Enables more advanced math tricks and simplify array manipulations\n",
    "from ipywidgets import interact # Enables interaction\n",
    "import matplotlib.pyplot as plt # Enables the ploting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enables interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables interactivity\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "The code below defines the activation functions and its respectively derivatives. In the context of the neural networks this functions are called activation functions.\n",
    "For more functions like that visit: https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates a list of activation functions and its derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some activations function and its respectives derivatives.\n",
    "Source: https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions\n",
    "\"\"\"\n",
    "\n",
    "############# Sigmoid ############\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + math.e**(-x))\n",
    "\n",
    "def sigmoidD(x): \n",
    "    y = sigmoid(x)\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "activations.append(('sigmoid',[sigmoid, sigmoidD]))\n",
    "\n",
    "\n",
    "############# Step ############\n",
    "def step(x):\n",
    "    return np.heaviside(x, 0)\n",
    "\n",
    "def stepD(x):\n",
    "    # This subfunction is used to\n",
    "    # modify regular numbers or\n",
    "    # numpy arrays\n",
    "    def f(a):\n",
    "        if a != 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return np.NAN\n",
    "    \n",
    "    # Checks if we are using an array\n",
    "    # or an regular number\n",
    "    if type(x) == np.ndarray:\n",
    "        # array\n",
    "        return np.array(list(map(f, x)))\n",
    "    else:\n",
    "        # regular number \n",
    "        return f(x)\n",
    "\n",
    "    \n",
    "activations.append(('step',[step, stepD]))\n",
    "\n",
    "\n",
    "############# Relu ############\n",
    "def relu(x):\n",
    "    # Checks if we are using an array\n",
    "    # or an regular number\n",
    "    if type(x) == np.ndarray:\n",
    "        # array\n",
    "        return np.fmax(x,0)\n",
    "    else:\n",
    "        # regular number \n",
    "        return max(x,0)\n",
    "\n",
    "\n",
    "def reluD(x):\n",
    "    # Checks if we are using an array\n",
    "    # or an regular number\n",
    "    if type(x) == np.ndarray:\n",
    "        # array\n",
    "        return np.heaviside(x, 1)\n",
    "    else:\n",
    "        # regular number \n",
    "        return 0 if x < 0 else 1\n",
    "\n",
    "    \n",
    "activations.append(('relu',[relu, reluD]))\n",
    "\n",
    "\n",
    "############# Hiperbolic tangent ############\n",
    "def hiperbolicTangent(x):\n",
    "    y1 = math.e**(x)\n",
    "    y2 = math.e**(-x)\n",
    "    return (y1 - y2) / (y1 + y2)\n",
    "\n",
    "def hiperbolicTangentD(x):\n",
    "    return 1 - hiperbolicTangent(x)**2\n",
    "\n",
    "activations.append(('hiperbolic tangent',[hiperbolicTangent, hiperbolicTangentD]))\n",
    "\n",
    "\n",
    "############# Softplus ############\n",
    "def softplus(x):\n",
    "    return np.log(1 + math.e**x)\n",
    "\n",
    "def softplusD(x):\n",
    "    return 1.0/(1.0 + math.e**(-x))\n",
    "\n",
    "activations.append(('softplus',[softplus, softplusD]))\n",
    "\n",
    "\n",
    "############# Leaky relu ############\n",
    "def leakyRelu(x):\n",
    "    # This subfunction is used to\n",
    "    # modify regular numbers or\n",
    "    # numpy arrays\n",
    "    def f(a):\n",
    "        if a < 0:\n",
    "            return 0.01 * a\n",
    "        else:\n",
    "            return a\n",
    "    \n",
    "    # Checks if we are using an array\n",
    "    # or an regular number\n",
    "    if type(x) == np.ndarray:\n",
    "        # array\n",
    "        return np.array(list(map(f, x)))\n",
    "    else:\n",
    "        # regular number \n",
    "        return f(x)\n",
    "\n",
    "    \n",
    "def leakyReluD(x):\n",
    "    # This subfunction is used to\n",
    "    # modify regular numbers or\n",
    "    # numpy arrays\n",
    "    def f(a):\n",
    "        if a < 0:\n",
    "            return 0.01\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # Checks if we are using an array\n",
    "    # or an regular number\n",
    "    if type(x) == np.ndarray:\n",
    "        # array\n",
    "        return np.array(list(map(f, x)))\n",
    "    else:\n",
    "        # regular number \n",
    "        return f(x)\n",
    "    \n",
    "activations.append(('leaky relu',[leakyRelu, leakyReluD]))\n",
    "\n",
    "\n",
    "############# Silu ############\n",
    "def silu(x):\n",
    "    return x/(1.0 + math.e**(-x))\n",
    "\n",
    "def siluD(x):\n",
    "    return (\n",
    "        (1 + math.e**(-x))\n",
    "        +\n",
    "        (x * math.e**(-x))\n",
    "    )/(\n",
    "        (1 + math.e**(-x))**2\n",
    "      )\n",
    "\n",
    "activations.append(('silu',[silu, siluD]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting parameters and function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Makes the plot bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to redraw the plot\n",
    "def plotNeuron(x = 0.0, lowerBound = -10, upperBound = 10, resolution=100, func = activations[0]):\n",
    "    f = func[0]\n",
    "    slopef = func[1]\n",
    "    \n",
    "    # evaluation of the function\n",
    "    y = f(x) # Evaluate function on point x\n",
    "    \n",
    "    xrange = np.linspace(lowerBound, upperBound, resolution) # Define the range of the plot\n",
    "    yrange = f(xrange) # Evaluate the function on all plot points\n",
    "\n",
    "    # This is an interesting approach on how to derive a function\n",
    "    # pretty clever! That is why this is still here\n",
    "    # h = 0.000000000001\n",
    "    # slope = (f(x+h)-f(x))/h # the tangent curve's angular coefficient\n",
    "\n",
    "    slope = slopef(x) # the tangent curve's angular coefficient\n",
    "    tangentY = y+slope*(xrange-x)  # tangent curve Ys\n",
    "\n",
    "    # Define the boundaries of the plot\n",
    "    plt.ylim([f(lowerBound) - 0.2, f(upperBound) + 0.2])\n",
    "    plt.xlim([lowerBound, upperBound])\n",
    "    \n",
    "    # Plot the curve\n",
    "    plt.plot(xrange, yrange)\n",
    "    \n",
    "    # Plot the tangent line\n",
    "    plt.plot(xrange, tangentY, 'C1--', linewidth = 2)\n",
    "    \n",
    "    # Plot the vertical line, dot and label\n",
    "    plt.axvline(x, color = 'c')\n",
    "    plt.scatter(x, y, color='C1', s=50)\n",
    "    plt.annotate('x:{:04.2f}, y:{:04.2f} -> slope:{:04.2f}'.format(x, y, slope), (lowerBound, f(upperBound) + 0.2))\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, this one enables the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744aed103bf64ae89dcb1afe197bb14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x', max=10.0, min=-10.0, step=1e-05), IntSlider(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotNeuron(x=0.0, lowerBound=-10, upperBound=10, resolution=100, func=('sigmoid', [<function sigmoid at 0x110739310>, <function sigmoidD at 0x1107393a0>]))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plotNeuron, x=(-10.0, 10.0, 0.00001),func=activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
