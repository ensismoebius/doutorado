@article{ParkHyeong-jun2023Mcoi,
pages = {1186594-1186594},
publisher = {Frontiers Media S.A},
title = {Multiclass classification of imagined speech EEG using noise-assisted multivariate empirical mode decomposition and multireceptive field convolutional neural network},
volume = {17},
year = {2023},
abstract = {Introduction In this study, we classified electroencephalography (EEG) data of imagined speech using signal decomposition and multireceptive convolutional neural network. The imagined speech EEG with five vowels /a/, /e/, /i/, /o/, and /u/, and mute (rest) sounds were obtained from ten study participants. Materials and methods First, two different signal decomposition methods were applied for comparison: noise-assisted multivariate empirical mode decomposition and wavelet packet decomposition. Six statistical features were calculated from the decomposed eight sub-frequency bands EEG. Next, all features obtained from each channel of the trial were vectorized and used as the input vector of classifiers. Lastly, EEG was classified using multireceptive field convolutional neural network and several other classifiers for comparison. Results We achieved an average classification rate of 73.09 and up to 80.41% in a multiclass (six classes) setup (Chance: 16.67%). In comparison with various other classifiers, significant improvements for other classifiers were achieved ( p -value < 0.05). From the frequency sub-band analysis, high-frequency band regions and the lowest-frequency band region contain more information about imagined vowel EEG data. The misclassification and classification rate of each vowel imaginary EEG was analyzed through a confusion matrix. Discussion Imagined speech EEG can be classified successfully using the proposed signal decomposition method and a convolutional neural network. The proposed classification method for imagined speech EEG can contribute to developing a practical imagined speech-based brain-computer interfaces system.},
author = {Park, Hyeong-jun and Lee, Boreom},
copyright = {Copyright © 2023 Park and Lee. 2023 Park and Lee},
issn = {1662-5161},
journal = {Frontiers in human neuroscience},
keywords = {brain-computer interfaces ; imagined speech EEG ; multiclass classification ; multireceptive field convolutional neural network ; Neuroscience ; noise-assisted empirical mode decomposition},
language = {eng},
}

@article{AbdulghaniMokhlesM2023ISCU,
pages = {649-},
publisher = {MDPI AG},
title = {Imagined Speech Classification Using EEG and Deep Learning},
volume = {10},
year = {2023},
number = {6},
abstract = {In this paper, we propose an imagined speech-based brain wave pattern recognition using deep learning. Multiple features were extracted concurrently from eight-channel electroencephalography (EEG) signals. To obtain classifiable EEG data with fewer sensors, we placed the EEG sensors on carefully selected spots on the scalp. To decrease the dimensions and complexity of the EEG dataset and to avoid overfitting during the deep learning algorithm, we utilized the wavelet scattering transformation. A low-cost 8-channel EEG headset was used with MATLAB 2023a to acquire the EEG data. The long-short term memory recurrent neural network (LSTM-RNN) was used to decode the identified EEG signals into four audio commands: up, down, left, and right. Wavelet scattering transformation was applied to extract the most stable features by passing the EEG dataset through a series of filtration processes. Filtration was implemented for each individual command in the EEG datasets. The proposed imagined speech-based brain wave pattern recognition approach achieved a 92.50% overall classification accuracy. This accuracy is promising for designing a trustworthy imagined speech-based brain-computer interface (BCI) future real-time systems. For better evaluation of the classification performance, other metrics were considered, and we obtained 92.74%, 92.50%, and 92.62% for precision, recall, and F1-score, respectively.},
author = {Abdulghani, Mokhles M and Walters, Wilbur L and Abed, Khalid H},
address = {Switzerland},
copyright = {COPYRIGHT 2023 MDPI AG},
issn = {2306-5354},
journal = {Bioengineering (Basel)},
keywords = {Accuracy ; Algorithms ; Bioengineering ; Brain ; Brain research ; brain–computer interface (BCI) ; Classification ; Computer applications ; Datasets ; Deep learning ; Design ; EEG ; EEG decoding ; Electroencephalography ; Filtration ; Human-computer interface ; imagined speech ; Implants ; inner speech ; Internet of Things ; LSTM ; Machine learning ; Methods ; Neural networks ; Pattern recognition ; Recurrent neural networks ; Research methodology ; Scattering ; Sensors ; Short term memory ; Signal classification ; Speech ; Systems design ; Transformation ; Voice recognition ; wavelet scattering transformation (WST)},
language = {eng},
}

@article{MahapatraNrushinghCharan2023Ecoi,
pages = {26040-},
publisher = {IOP Publishing},
title = {EEG-based classification of imagined digits using a recurrent neural network},
volume = {20},
year = {2023},
number = {2},
abstract = {In recent years, imagined speech brain-computer (machine) interface applications have been an important field of study that can improve the lives of patients with speech problems through alternative verbal communication. This study aims to classify the imagined speech of numerical digits from electroencephalography (EEG) signals by exploiting the past and future temporal characteristics of the signal using several deep learning models.
This study proposes a methodological combination of EEG signal processing techniques and deep learning models for the recognition of imagined speech signals. EEG signals were filtered and preprocessed using the discrete wavelet transform to remove artifacts and retrieve feature information. To classify the preprocessed imagined speech neural signals, multiple versions of multilayer bidirectional recurrent neural networks were used.
The method is examined by leveraging MUSE and EPOC signals from MNIST imagined digits in the MindBigData open-access database. The presented methodology's classification performance accuracy was noteworthy, with the model's multiclass overall classification accuracy reaching a maximum of 96.18% on MUSE signals and 71.60% on EPOC signals.
This study shows that the proposed signal preprocessing approach and the stacked bidirectional recurrent network model are suitable for extracting the high temporal resolution of EEG signals in order to classify imagined digits, indicating the unique neural identity of each imagined digit class that distinguishes it from the others.},
author = {Mahapatra, Nrushingh Charan and Bhuyan, Prachet},
address = {England},
copyright = {2023 IOP Publishing Ltd},
issn = {1741-2560},
journal = {Journal of neural engineering},
keywords = {Algorithms ; Alprostadil ; bidirectional recurrent neural network ; Brain-Computer Interfaces ; deep learning ; electroencephalography (EEG) ; Electroencephalography - methods ; Humans ; imagined speech ; Neural Networks, Computer ; signal processing},
language = {eng},
}

@article{KambleAshwin2023ORDW,
pages = {1-10},
publisher = {IEEE},
title = {Optimized Rational Dilation Wavelet Transform for Automatic Imagined Speech Recognition},
volume = {72},
year = {2023},
abstract = {Imagined speech is a process in which a person imagines words without saying them. Electroencephalogram (EEG)-based brain-computer interface (BCI) systems help in automatically identifying imagined speech to facilitate persons with severe brain disorders. Extracting meaningful information from the raw EEG signal is a challenging task due to the nonstationary nature of EEG signals. Decomposing a signal into several sub-bands (SBs) using rational dilation wavelet transform (RADWT) requires selecting predefined factual parameters, which is an arduous task. The main objective of this study is to propose an adaptive RADWT method capable of decomposing EEG signals by adaptively selecting the tuning parameters and classifying the EEG signals into distinct categories. The optimum tuning parameters of RADWT are obtained using particle swarm optimization (PSO) and used to decompose the EEG signals into several SBs. Several statistical features are elicited from each SB and used to input six different machine learning (ML) algorithms. This work uses a 64-channel EEG dataset recorded from 15 healthy people for three categories: long words, short words, and vowels. The performance of the proposed AISR system is evaluated using seven performance evaluation metrics: accuracy, recall, precision, Cohen's kappa, F1-score, and area under the curve. The proposed system achieved the average classification accuracies of 87.26% ± 1.12%, 89.23% ± 0.95%, 95.5% ± 0.68%, and 92.16% ± 0.83% for long words, short-long words, short words, and vowels, respectively. When compared with the existing state-of-the-art, the proposed non-parametric decomposition approach and the Bagging algorithm achieved a 3%-5% improvement. The performance of the proposed method is validated using an open-access dataset.},
author = {Kamble, Ashwin and Ghare, Pradnya H. and Kumar, Vinay},
address = {New York},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2023},
issn = {0018-9456},
journal = {IEEE transactions on instrumentation and measurement},
keywords = {Algorithms ; Automatic imagined speech recognition (AISR) ; Classification algorithms ; Datasets ; Decomposition ; Dilation ; electroencephalogram (EEG) ; Electroencephalography ; Feature extraction ; Human-computer interface ; Machine learning ; machine learning (ML) algorithms ; Machine learning algorithms ; Parameters ; Particle swarm optimization ; particle swarm optimization (PSO) ; Performance evaluation ; rational dilation wavelet transform (RADWT) ; Signal classification ; Speech recognition ; Support vector machines ; Tuning ; Vowels ; Wavelet transforms ; Words (language)},
language = {eng},
}

@article{KambleAshwin2023DBfA,
pages = {1-10},
publisher = {IEEE},
title = {Deep-Learning-Based BCI for Automatic Imagined Speech Recognition Using SPWVD},
volume = {72},
year = {2023},
abstract = {The electroencephalogram (EEG)-based brain-computer interface (BCI) has potential applications in neuroscience and rehabilitation. It benefits a person with neurological impairment to communicate their thoughts to the external world without using any appendages. Decoding imagined speech had limited success, mainly because neural signals are weak and more variable than overt speech, hence challenging to decode by machine-learning (ML)-based algorithms. In recent years, deep learning (DL) with convolutional neural networks (CNNs) has transformed computer vision and can perform pattern recognition better than the traditional ML-based algorithms. The objective of this article is to design a smoothed pseudo-Wigner-Ville distribution (SPWVD) and CNN-based automatic imagined speech recognition (AISR) system to recognize imagined words. This article uses a publically available 64-channel EEG dataset, collected from 15 healthy subjects for three categories: long words, short words, and vowels. The EEG signals were transformed into time-frequency representation (TFR) using SPWVD, which are used as an input to CNN such that the EEG dataset was identified and classified into binary and multiclass categories. In addition, the CNN model was optimized using a recently developed Keras-tuner library to achieve optimal performance. The performance of the SPWVD-CNN-driven AISR system is evaluated using seven performance evaluation metrics: accuracy (ACC), recall (REC), precision (PREC), Mathew's correlation coefficient (MCC), Cohen's kappa (<inline-formula> <tex-math notation="LaTeX">\kappa </tex-math></inline-formula>), F1-score, and area under the curve (AUC). It is found that the proposed system achieved the maximum classification ACCs of 94.82%, 94.26%, 94.68%, and 84.50% for long words, short-long words, short words, and vowels, respectively. The proposed AISR strengthens the possibility of using imagined speech recognition as a future BCI application.},
author = {Kamble, Ashwin and Ghare, Pradnya H. and Kumar, Vinay},
address = {New York},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2023},
issn = {0018-9456},
journal = {IEEE transactions on instrumentation and measurement},
keywords = {Algorithms ; Appendages ; Artificial neural networks ; Automatic imagined speech recognition (AISR) ; brain–computer interface (BCI) ; Computer vision ; Continuous wavelet transforms ; Convolutional neural networks ; convolutional neural networks (CNNs) ; Correlation coefficients ; Datasets ; Decoding ; Deep learning ; deep learning (DL) ; electroencephalogram (EEG) ; Electroencephalography ; Feature extraction ; Human-computer interface ; Image recognition ; Machine learning ; Pattern recognition ; Performance evaluation ; Rehabilitation ; smoothed pseudo-Wigner-Ville distribution (SPWVD) ; Speech recognition ; Time-frequency analysis ; time-frequency representation (TFR) ; Voice recognition ; Vowels ; Words (language)},
language = {eng},
}

@article{ShahUzair2022TRoA,
pages = {6975-},
publisher = {MDPI AG},
title = {The Role of Artificial Intelligence in Decoding Speech from EEG Signals: A Scoping Review},
volume = {22},
year = {2022},
number = {18},
abstract = {Background: Brain traumas, mental disorders, and vocal abuse can result in permanent or temporary speech impairment, significantly impairing one’s quality of life and occasionally resulting in social isolation. Brain–computer interfaces (BCI) can support people who have issues with their speech or who have been paralyzed to communicate with their surroundings via brain signals. Therefore, EEG signal-based BCI has received significant attention in the last two decades for multiple reasons: (i) clinical research has capitulated detailed knowledge of EEG signals, (ii) inexpensive EEG devices, and (iii) its application in medical and social fields. Objective: This study explores the existing literature and summarizes EEG data acquisition, feature extraction, and artificial intelligence (AI) techniques for decoding speech from brain signals. Method: We followed the PRISMA-ScR guidelines to conduct this scoping review. We searched six electronic databases: PubMed, IEEE Xplore, the ACM Digital Library, Scopus, arXiv, and Google Scholar. We carefully selected search terms based on target intervention (i.e., imagined speech and AI) and target data (EEG signals), and some of the search terms were derived from previous reviews. The study selection process was carried out in three phases: study identification, study selection, and data extraction. Two reviewers independently carried out study selection and data extraction. A narrative approach was adopted to synthesize the extracted data. Results: A total of 263 studies were evaluated; however, 34 met the eligibility criteria for inclusion in this review. We found 64-electrode EEG signal devices to be the most widely used in the included studies. The most common signal normalization and feature extractions in the included studies were the bandpass filter and wavelet-based feature extraction. We categorized the studies based on AI techniques, such as machine learning and deep learning. The most prominent ML algorithm was a support vector machine, and the DL algorithm was a convolutional neural network. Conclusions: EEG signal-based BCI is a viable technology that can enable people with severe or temporal voice impairment to communicate to the world directly from their brain. However, the development of BCI technology is still in its infancy.},
author = {Shah, Uzair and Alzubaidi, Mahmood and Mohsen, Farida and Abd-Alrazaq, Alaa and Alam, Tanvir and Househ, Mowafa},
address = {Basel},
copyright = {COPYRIGHT 2022 MDPI AG},
issn = {1424-8220},
journal = {Sensors (Basel, Switzerland)},
keywords = {Algorithms ; Artificial intelligence ; Bandpass filters ; Communication ; Data acquisition ; Deep learning ; Digital systems ; Discriminant analysis ; electroencephalogram ; Electroencephalography ; Electromyography ; Feature extraction ; Human-computer interface ; imagine speech ; Impairment ; Machine learning ; Medical research ; Medicine, Experimental ; Mental disorders ; Neural networks ; Review ; sensors ; signals ; Speech ; speech decoding ; Support vector machines ; Voice communication},
language = {eng},
}

@article{MahapatraNrushinghCharan2022MCoI,
pages = {1-10},
publisher = {Hindawi},
title = {Multiclass Classification of Imagined Speech Vowels and Words of Electroencephalography Signals Using Deep Learning},
volume = {2022},
year = {2022},
abstract = {The paper’s emphasis is on the imagined speech decoding of electroencephalography (EEG) neural signals of individuals in accordance with the expansion of the brain-computer interface to encompass individuals with speech problems encountering communication challenges. Decoding an individual’s imagined speech from nonstationary and nonlinear EEG neural signals is a complex task. Related research work in the field of imagined speech has revealed that imagined speech decoding performance and accuracy require attention to further improve. The evolution of deep learning technology increases the likelihood of decoding imagined speech from EEG signals with enhanced performance. We proposed a novel supervised deep learning model that combined the temporal convolutional networks and the convolutional neural networks with the intent of retrieving information from the EEG signals. The experiment was carried out using an open-access dataset of fifteen subjects’ imagined speech multichannel signals of vowels and words. The raw multichannel EEG signals of multiple subjects were processed using discrete wavelet transformation technique. The model was trained and evaluated using the preprocessed signals, and the model hyperparameters were adjusted to achieve higher accuracy in the classification of imagined speech. The experiment results demonstrated that the multiclass imagined speech classification of the proposed model exhibited a higher overall accuracy of 0.9649 and a classification error rate of 0.0350. The results of the study indicate that individuals with speech difficulties might well be able to leverage a noninvasive EEG-based imagined speech brain-computer interface system as one of the long-term alternative artificial verbal communication mediums.},
author = {Mahapatra, Nrushingh Charan and Bhuyan, Prachet},
address = {New York},
copyright = {Copyright © 2022 Nrushingh Charan Mahapatra and Prachet Bhuyan.},
issn = {1687-5893},
journal = {Advances in human-computer interaction},
keywords = {Accuracy ; Analysis ; Artificial neural networks ; Brain ; Brain research ; Classification ; Communication ; Computer applications ; Deep learning ; Discrete Wavelet Transform ; Discriminant analysis ; EEG ; Electroencephalography ; Eye movements ; Human-computer interface ; Implants ; Information retrieval ; Machine learning ; Multichannel communication ; Muscle function ; Neural networks ; Performance enhancement ; Signal processing ; Speech ; Support vector machines ; Verbal communication ; Vowels ; Words (language)},
language = {eng},
}

@article{AgarwalPrabhakar2022Ebia,
pages = {111-122},
publisher = {John Wiley & Sons, Inc},
title = {Electroencephalography based imagined alphabets classification using spatial and time‐domain features},
volume = {32},
year = {2022},
number = {1},
abstract = {Imagined speech is a neuro‐paradigm that can provide an alternative communication channel for patients in a locked‐in syndrome state. We have performed an experiment in which a 32 channel industry‐standard electroencephalography (EEG) device was used to record 26 imagined English alphabets from 13 subjects. We denoised the imagined signals by discrete wavelet transform and extracted the spatial filters by common spatial pattern method, and time‐domain features. Spatial features when classified with linear support vector machine, and time‐domain features classified by random forest gave the best results. Alpha, beta, and theta bands could classify imagined alphabets better than other bands and had average classification accuracies of 88.59%, 87.39%, and 88.97%, respectively by using spatial features and 81.88%, 76.72%, and 79.25%, respectively, by time‐domain features. The grand average accuracies of all the 26 alphabets in six EEG frequency bands was found to be 77.97% in a subject independent binary classification framework.},
author = {Agarwal, Prabhakar and Kumar, Sandeep},
address = {Hoboken, USA},
copyright = {2021 Wiley Periodicals LLC.},
issn = {0899-9457},
journal = {International journal of imaging systems and technology},
keywords = {Artificial intelligence ; Brain–computer interface ; Classification ; Computer science ; Discrete Wavelet Transform ; Domains ; electroencephalogram ; Electroencephalography ; Electromagnetic wave filters ; Frequencies ; imagined alphabets ; Pattern method (forecasting) ; Pattern recognition ; Spatial filtering ; Support vector machines ; Time domain ; Wavelet transforms},
language = {eng},
}

@article{SarmientoLuisCarlos2021RoES,
pages = {6503-},
publisher = {MDPI AG},
title = {Recognition of EEG Signals from Imagined Vowels Using Deep Learning Methods},
volume = {21},
year = {2021},
number = {19},
abstract = {The use of imagined speech with electroencephalographic (EEG) signals is a promising field of brain-computer interfaces (BCI) that seeks communication between areas of the cerebral cortex related to language and devices or machines. However, the complexity of this brain process makes the analysis and classification of this type of signals a relevant topic of research. The goals of this study were: to develop a new algorithm based on Deep Learning (DL), referred to as CNNeeg1-1, to recognize EEG signals in imagined vowel tasks; to create an imagined speech database with 50 subjects specialized in imagined vowels from the Spanish language (/a/,/e/,/i/,/o/,/u/); and to contrast the performance of the CNNeeg1-1 algorithm with the DL Shallow CNN and EEGNet benchmark algorithms using an open access database (BD1) and the newly developed database (BD2). In this study, a mixed variance analysis of variance was conducted to assess the intra-subject and inter-subject training of the proposed algorithms. The results show that for intra-subject training analysis, the best performance among the Shallow CNN, EEGNet, and CNNeeg1-1 methods in classifying imagined vowels (/a/,/e/,/i/,/o/,/u/) was exhibited by CNNeeg1-1, with an accuracy of 65.62% for BD1 database and 85.66% for BD2 database.},
author = {Sarmiento, Luis Carlos and Villamizar, Sergio and López, Omar and Collazos, Ana Claros and Sarmiento, Jhon and Rodríguez, Jan Bacca},
address = {Basel},
copyright = {2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {1424-8220},
journal = {Sensors (Basel, Switzerland)},
keywords = {Accuracy ; Algorithms ; Artificial intelligence ; Benchmark (computing) ; Brain research ; brain-computer interface (BCI) ; Brain–computer interface ; Cerebral cortex ; Classification ; Computer science ; Contrast (statistics) ; convolutional neural networks (CNN) ; Deep learning ; Eigenvalues ; Electroencephalography ; Field (computer science) ; Human-computer interface ; Imagined speech ; Neural networks ; Signal processing ; Signal to noise ratio ; Spanish language ; Speech ; Speech recognition ; Standard deviation ; Training analysis ; Variance analysis ; Vowel ; Vowels ; Wavelet transforms},
language = {eng},
}

@article{UllahSadiq2021Icrt,
pages = {1167-1183},
publisher = {Springer Berlin Heidelberg},
title = {Imagined character recognition through EEG signals using deep convolutional neural network},
volume = {59},
year = {2021},
number = {5},
abstract = {Electroencephalography (EEG)-based brain computer interface (BCI) enables people to interact directly with computing devices through their brain signals. A BCI typically interprets EEG signals to reflect the user’s intent or other mental activity. Motor imagery (MI) is a commonly used technique in BCIs where a user is asked to imagine moving certain part of the body such as a hand or a foot. By correctly interpreting the signal, one can perform a multitude of tasks such as controlling wheel chair, playing computer games, or even typing text. However, the use of motor-imagery-based BCIs outside the laboratory environment is limited due to the lack of their reliability. This work focuses on another kind of mental imagery, namely, the visual imagery (VI). VI is the manipulation of visual information that comes from memory. This work presents a deep convolutional neural network (DCNN)–based system for the recognition of visual/mental imagination of English alphabets so as to enable typing directly via brain signals. The DCNN learns to extract the spatial features hidden in the EEG signal. As opposed to many deep neural networks that use raw EEG signals for classification, this work transforms the raw signals into band powers using Morlet wavelet transformation. The proposed approach is evaluated on two publicly available benchmark MI-EEG datasets and a visual imagery dataset specifically collected for this work. The obtained results demonstrate that the proposed model performs better than the existing state-of-the-art methods for MI-EEG classification and yields an average accuracy of 99.45% on the two public MI-EEG datasets. The model also achieves an average recognition rate of 95.2% for the 26 English-language alphabets.
Graphical abstract
Overall working of the proposed solution for imagined character recognition through EEG signals},
author = {Ullah, Sadiq and Halim, Zahid},
address = {Berlin/Heidelberg},
copyright = {International Federation for Medical and Biological Engineering 2021},
issn = {0140-0118},
journal = {Medical & biological engineering & computing},
keywords = {Alphabets ; Artificial intelligence ; Artificial neural networks ; Biomedical and Life Sciences ; biomedical engineering ; Biomedical Engineering and Bioengineering ; Biomedicine ; Brain ; Brain–computer interface ; Character recognition ; Classification ; clinical medicine ; Computer & video games ; Computer Applications ; Computer science ; Convolutional neural network ; Datasets ; Deep learning ; EEG ; Electroencephalography ; engineering and technology ; Feature extraction ; Human Physiology ; Human-computer interface ; Image manipulation ; Imaging ; Implants ; medical and health sciences ; medical engineering ; Mental image ; Mental task performance ; Morlet wavelet ; Motor imagery ; Neural networks ; nuclear medicine & medical imaging ; Original Article ; Radiology ; Signal classification ; Speech recognition ; Supervised learning ; Typing ; Visual system ; Wavelet transforms},
language = {eng},
}

@article{Hernández-Del-ToroTonatiuh2021TaEB,
publisher = {Cornell University Library, arXiv.org},
title = {Toward asynchronous EEG-based BCI: Detecting imagined words segments in continuous EEG signals},
year = {2021},
abstract = {An asynchronous Brain--Computer Interface (BCI) based on imagined speech is a tool that allows to control an external device or to emit a message at the moment the user desires to by decoding EEG signals of imagined speech. In order to correctly implement these types of BCI, we must be able to detect from a continuous signal, when the subject starts to imagine words. In this work, five methods of feature extraction based on wavelet decomposition, empirical mode decomposition, frequency energies, fractal dimension and chaos theory features are presented to solve the task of detecting imagined words segments from continuous EEG signals as a preliminary study for a latter implementation of an asynchronous BCI based on imagined speech. These methods are tested in three datasets using four different classifiers and the higher F1 scores obtained are 0.73, 0.79, and 0.68 for each dataset, respectively. This results are promising to build a system that automatizes the segmentation of imagined words segments for latter classification.},
author = {Hernández-Del-Toro, Tonatiuh and Reyes-García, Carlos A and Villaseñor-Pineda, Luis},
address = {Ithaca},
copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Chaos theory ; Computer Science - Human-Computer Interaction ; Computer Science - Learning ; Datasets ; Electroencephalography ; Feature extraction ; Fractal geometry ; Human-computer interface ; Segmentation ; Segments ; Speech},
language = {eng},
}

@article{Hernández-Del-ToroTonatiuh2021TaEB,
pages = {102351-},
publisher = {Elsevier Ltd},
title = {Toward asynchronous EEG-based BCI: Detecting imagined words segments in continuous EEG signals},
volume = {65},
year = {2021},
abstract = {An asynchronous Brain–Computer Interface (BCI) based on imagined speech is a tool that allows to control an external device or to emit a message at the moment the user desires to by decoding EEG signals of imagined speech. In order to correctly implement these types of BCI, we must be able to detect from a continuous signal, when the subject starts to imagine words. In this work, five methods of feature extraction based on wavelet decomposition, empirical mode decomposition, frequency energies, fractal dimension and chaos theory features are presented to solve the task of detecting imagined words segments from continuous EEG signals as a preliminary study for a latter implementation of an asynchronous BCI based on imagined speech. These methods are tested in three datasets using four different classifiers and the higher F1scores obtained are 0.73, 0.79, and 0.68 for each dataset, respectively. This results are promising to build a system that automatizes the segmentation of imagined words segments for latter classification.
•A new method for detecting imagined words segments in EEG signals is described.•Feature sets based on DWT, EMD, frequency energies, and chaos theory were evaluated.•The experimental scheme for detecting imagined word segments is subject-specific.•The system is being designed to be a module of a BCI based on imagined words.},
author = {Hernández-Del-Toro, Tonatiuh and Reyes-García, Carlos A. and Villaseñor-Pineda, Luis},
copyright = {2020 Elsevier Ltd},
issn = {1746-8094},
journal = {Biomedical signal processing and control},
keywords = {Asynchronous BCI ; Imagined speech ; Signal processing},
language = {eng},
}

@article{P.P.Mini2021Wfso,
pages = {102218-},
publisher = {Elsevier Ltd},
title = {Wavelet feature selection of audio and imagined/vocalized EEG signals for ANN based multimodal ASR system},
volume = {63},
year = {2021},
abstract = {Human-Machine Interaction (HMI) systems demand the use of multiple modalities for correct interaction. Research on these systems started with audio signals for speech recognition and now progressing towards co-operation of other biosignals. Thus, the paper presents an Automatic Speech Recognition (ASR) system based on a single and multiple modalities that include audio and Electroencephalogram (EEG) signals to explore speech recognition. It extracts speech information concealed in audio and ten channels of imagined EEG (EEG-i) & vocalized EEG (EEG-v) signals. Three Wavelet Transform (WT) methods - Discrete Wavelet Transform (DWT), Wavelet Packet Decomposition (WPD) & hybrid of DWT & WPD (DWPD) with four-level decomposition is used to transform the signals into WT coefficients. Then, six statistical parameters are computed from WT coefficients to generate 63 (26-1) feature vectors for each method. An exhaustive search from 63 feature vectors is conducted to determine the best parameter combination that attains good accuracy with ANN classifier. Then, accuracy is improved with 5 level decomposition on WPD coefficients along with the best parameter combination. Results include the accuracy of unimodal ASR & multimodal ASR. WPD method achieved best accuracy as 74.48%, 56.29%, 42.02%, 77.97% & 78.90% for multiclass classification of prompts+words based on audio, EEG-i, EEG-v, audio + EEG-i & audio + EEG-v respectively. It indicates that speech recognition is possible from EEG signals & the fusion of audio with EEG enhances the recognition rate of audio & EEG. The results also show that the proposed method outperforms other methods in the area.
•A multimodal ASR system using audio and EEG is proposed to explore speech recognition.•DWT, WPD and DWPD along with 6 statistical parameters are used for feature extraction.•ANN is used for classification•Proposed results are superior over existing works.•It is applicable as BCI devices.},
author = {P.P., Mini and Thomas, Tessamma and Gopikakumari, R.},
copyright = {2020 Elsevier Ltd},
issn = {1746-8094},
journal = {Biomedical signal processing and control},
keywords = {ANN ; Audio signal ; biomedical engineering ; clinical medicine ; Computer science ; Discrete wavelet transform ; EEG ; engineering and technology ; Feature selection ; Feature vector ; Imagined speech recognition ; medical and health sciences ; medical engineering ; Multiclass classification ; neurology & neurosurgery ; Speech recognition ; Vocalized speech recognition ; Wavelet ; Wavelet packet decomposition ; Wavelet transform},
language = {eng},
}

@article{PanachakelJerrinThomas2021DISF,
pages = {135371-135383},
publisher = {IEEE},
title = {Decoding Imagined Speech From EEG Using Transfer Learning},
volume = {9},
year = {2021},
abstract = {We present a transfer learning-based approach for decoding imagined speech from electroencephalogram (EEG). Features are extracted simultaneously from multiple EEG channels, rather than separately from individual channels. This helps in capturing the interrelationships between the cortical regions. To alleviate the problem of lack of enough data for training deep networks, sliding window-based data augmentation is performed. Mean phase coherence and magnitude-squared coherence, two popular measures used in EEG connectivity analysis, are used as features. These features are compactly arranged, exploiting their symmetry, to obtain a three dimensional "image-like" representation. The three dimensions of this matrix correspond to the alpha, beta and gamma EEG frequency bands. A deep network with ResNet50 as the base model is used for classifying the imagined prompts. The proposed method is tested on the publicly available ASU dataset of imagined speech EEG, comprising four different types of prompts. The accuracy of decoding the imagined prompt varies from a minimum of 79.7% for vowels to a maximum of 95.5% for short-long words across the various subjects. The accuracies obtained are better than the state-of-the-art methods, and the technique is good in decoding prompts of different complexities.},
author = {Panachakel, Jerrin Thomas and Ganesan, Ramakrishnan Angarai},
address = {Piscataway},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2021},
issn = {2169-3536},
journal = {IEEE access},
keywords = {Brain–computer interface ; Channels ; Coherence ; Coherence (statistics) ; Computer science ; Connectivity analysis ; Decoding ; Decoding methods ; Discrete wavelet transforms ; electroencephalogram ; Electroencephalography ; Feature extraction ; Frequencies ; Imagined speech ; Learning ; Phase coherence ; Representation (mathematics) ; Sliding window protocol ; Speech ; speech imagery ; Speech recognition ; Transfer learning ; Transfer of learning ; Transforms},
language = {eng},
}

@article{TammMarkus-Oliver2020CoVf,
pages = {46-},
publisher = {MDPI AG},
title = {Classification of Vowels from Imagined Speech with Convolutional Neural Networks},
volume = {9},
year = {2020},
number = {2},
abstract = {Imagined speech is a relatively new electroencephalography (EEG) neuro-paradigm, which has seen little use in Brain-Computer Interface (BCI) applications. Imagined speech can be used to allow physically impaired patients to communicate and to use smart devices by imagining desired commands and then detecting and executing those commands in a smart device. The goal of this research is to verify previous classification attempts made and then design a new, more efficient neural network that is noticeably less complex (fewer number of layers) that still achieves a comparable classification accuracy. The classifiers are designed to distinguish between EEG signal patterns corresponding to imagined speech of different vowels and words. This research uses a dataset that consists of 15 subjects imagining saying the five main vowels (a, e, i, o, u) and six different words. Two previous studies on imagined speech classifications are verified as those studies used the same dataset used here. The replicated results are compared. The main goal of this study is to take the proposed convolutional neural network (CNN) model from one of the replicated studies and make it much more simpler and less complex, while attempting to retain a similar accuracy. The pre-processing of data is described and a new CNN classifier with three different transfer learning methods is described and used to classify EEG signals. Classification accuracy is used as the performance metric. The new proposed CNN, which uses half as many layers and less complex pre-processing methods, achieved a considerably lower accuracy, but still managed to outperform the initial model proposed by the authors of the dataset by a considerable margin. It is recommended that further studies investigating classifying imagined speech should use more data and more powerful machine learning techniques. Transfer learning proved beneficial and should be used to improve the effectiveness of neural networks.},
author = {Tamm, Markus-Oliver and Muhammad, Yar and Muhammad, Naveed},
address = {Basel},
copyright = {COPYRIGHT 2020 MDPI AG},
issn = {2073-431X},
journal = {Computers (Basel)},
keywords = {Accuracy ; Applied research ; Artificial neural networks ; Classification ; Classifiers ; convolutional neural networks ; Datasets ; Deep learning ; Discriminant analysis ; EEG ; Electroencephalography ; Electronic devices ; Human-computer interface ; imagined speech ; Machine learning ; Methods ; Neural networks ; Signal classification ; Speech ; Studies ; Support vector machines ; transfer learning ; Vowels ; Wavelet transforms ; Words (language)},
language = {eng},
}

@article{JerrinThomasPanachakel2020ANDL,
publisher = {Cornell University Library, arXiv.org},
title = {A Novel Deep Learning Architecture for Decoding Imagined Speech from EEG},
year = {2020},
abstract = {The recent advances in the field of deep learning have not been fully utilised for decoding imagined speech primarily because of the unavailability of sufficient training samples to train a deep network. In this paper, we present a novel architecture that employs deep neural network (DNN) for classifying the words "in" and "cooperate" from the corresponding EEG signals in the ASU imagined speech dataset. Nine EEG channels, which best capture the underlying cortical activity, are chosen using common spatial pattern (CSP) and are treated as independent data vectors. Discrete wavelet transform (DWT) is used for feature extraction. To the best of our knowledge, so far DNN has not been employed as a classifier in decoding imagined speech. Treating the selected EEG channels corresponding to each imagined word as independent data vectors helps in providing sufficient number of samples to train a DNN. For each test trial, the final class label is obtained by applying a majority voting on the classification results of the individual channels considered in the trial. We have achieved accuracies comparable to the state-of-the-art results. The results can be further improved by using a higher-density EEG acquisition system in conjunction with other deep learning techniques such as long short-term memory.},
author = {Jerrin Thomas Panachakel and Ramakrishnan, A G and Ananthapadmanabha, T V},
address = {Ithaca},
copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Architecture ; Artificial neural networks ; Channels ; Classification ; Computer Science - Learning ; Deep learning ; Discrete Wavelet Transform ; Electroencephalography ; Feature extraction ; Machine learning ; Speech ; Statistics - Machine Learning ; Wavelet transforms},
language = {eng},
}

@article{MoctezumaLuisAlfredo2019SiuE,
pages = {201-208},
publisher = {Elsevier Ltd},
title = {Subjects identification using EEG-recorded imagined speech},
volume = {118},
year = {2019},
abstract = {•An EEG-based imagined speech system was assessed for subjects recognition.•More than one word can be used as a password without decreasing the method performance.•Impact of instances/channels reduction and feature extraction techniques were studied.
Due to the problems presented in current traditional/biometric security systems, the interest to use new security systems, have been increasing. This paper explores the use of brain signals EEG-based during imagined speech in order to use it as a new biometric measure for Subjects identification and thus create a new biometric security system. The main contribution of this paper are two methods for feature extraction, first to improve the signal-to-noise ratio the Common Average Reference was applied. The first method was based on Discrete Wavelet Transform, and the second method was based on statistical features directly from the raw signal. The proposed methods were tested in a dataset of 27 Subjects who performed 33 repetitions of 5 imagined words in Spanish. The results show the feasibility of the task with accurate identification of the Subject, regardless of the imagined word used and using a commercial EEG system (EMOTIV EPOC). In addition, the scope of the method is displayed by decreasing the training data, as well as the number of active sensors for the identification task. Using the proposed method with future improvements and implementing it in a low-cost device can be a new and valuable biometric security system.},
author = {Moctezuma, Luis Alfredo and Torres-García, Alejandro A. and Villaseñor-Pineda, Luis and Carrillo, Maya},
address = {New York},
copyright = {2018 Elsevier Ltd},
issn = {0957-4174},
journal = {Expert systems with applications},
keywords = {artificial intelligence & image processing ; Biometrics ; Brain ; Computer science ; Discrete Wavelet Transform ; electrical engineering, electronic engineering, information engineering ; Electroencephalograms (EEG) ; Electroencephalography ; engineering and technology ; Feature extraction ; Identification ; Identification (information) ; Imagined speech ; industrial biotechnology ; industrial engineering & automation ; Security systems ; Sensors ; Speech recognition ; Subject identification ; Task (project management) ; Word (computer architecture)},
language = {eng},
}

