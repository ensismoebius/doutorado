@article{javed2023neuroanatomy,
	title={Neuroanatomy, Wernicke Area},
	author={Javed, K and Reddy, V and Das, M and et al.},
	journal={StatPearls [Internet]},
	year={2024},
	month={Jan},
	publisher={StatPearls Publishing},
	address={Treasure Island (FL)},
	note={[Updated 2023 Jul 24]. Available from: \url{https://www.ncbi.nlm.nih.gov/books/NBK533001/}}
}

@misc{panachakel2020novel,
	title={A Novel Deep Learning Architecture for Decoding Imagined Speech from EEG}, 
	author={Jerrin Thomas Panachakel and A. G. Ramakrishnan and T. V. Ananthapadmanabha},
	year={2020},
	eprint={2003.09374},
	archivePrefix={arXiv},
	primaryClass={id='eess.SP' full_name='Signal Processing' is_active=True alt_name=None in_archive='eess' is_general=False description='Theory, algorithms, performance analysis and applications of signal and data analysis, including physical modeling, processing, detection and parameter estimation, learning, mining, retrieval, and information extraction. The term "signal" includes speech, audio, sonar, radar, geophysical, physiological, (bio-) medical, image, video, and multimodal natural and man-made signals, including communication signals and data. Topics of interest include: statistical signal processing, spectral estimation and system identification; filter design, adaptive filtering / stochastic learning; (compressive) sampling, sensing, and transform-domain methods including fast algorithms; signal processing for machine learning and machine learning for signal processing applications; in-network and graph signal processing; convex and nonconvex optimization methods for signal processing applications; radar, sonar, and sensor array beamforming and direction finding; communications signal processing; low power, multi-core and system-on-chip signal processing; sensing, communication, analysis and optimization for cyber-physical systems such as power grids and the Internet of Things.'}
}

@Article{bioengineering10060649,
	AUTHOR = {Abdulghani, Mokhles M. and Walters, Wilbur L. and Abed, Khalid H.},
	TITLE = {Imagined Speech Classification Using EEG and Deep Learning},
	JOURNAL = {Bioengineering},
	VOLUME = {10},
	YEAR = {2023},
	NUMBER = {6},
	ARTICLE-NUMBER = {649},
	URL = {https://www.mdpi.com/2306-5354/10/6/649},
	PubMedID = {37370580},
	ISSN = {2306-5354},
	ABSTRACT = {In this paper, we propose an imagined speech-based brain wave pattern recognition using deep learning. Multiple features were extracted concurrently from eight-channel electroencephalography (EEG) signals. To obtain classifiable EEG data with fewer sensors, we placed the EEG sensors on carefully selected spots on the scalp. To decrease the dimensions and complexity of the EEG dataset and to avoid overfitting during the deep learning algorithm, we utilized the wavelet scattering transformation. A low-cost 8-channel EEG headset was used with MATLAB 2023a to acquire the EEG data. The long-short term memory recurrent neural network (LSTM-RNN) was used to decode the identified EEG signals into four audio commands: up, down, left, and right. Wavelet scattering transformation was applied to extract the most stable features by passing the EEG dataset through a series of filtration processes. Filtration was implemented for each individual command in the EEG datasets. The proposed imagined speech-based brain wave pattern recognition approach achieved a 92.50% overall classification accuracy. This accuracy is promising for designing a trustworthy imagined speech-based brain–computer interface (BCI) future real-time systems. For better evaluation of the classification performance, other metrics were considered, and we obtained 92.74%, 92.50%, and 92.62% for precision, recall, and F1-score, respectively.},
	DOI = {10.3390/bioengineering10060649}
}

@article{doi:10.1073/pnas.1414491112,
	author = {Adeen Flinker  and Anna Korzeniewska  and Avgusta Y. Shestyuk  and Piotr J. Franaszczuk  and Nina F. Dronkers  and Robert T. Knight  and Nathan E. Crone },
	title = {Redefining the role of Broca’s area in speech},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {112},
	number = {9},
	pages = {2871-2875},
	year = {2015},
	doi = {10.1073/pnas.1414491112},
	URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1414491112},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1414491112},
	abstract = {For over a century neuroscientists have debated the dynamics by which human cortical language networks allow words to be spoken. Although it is widely accepted that Broca’s area in the left inferior frontal gyrus plays an important role in this process, it was not possible, until recently, to detail the timing of its recruitment relative to other language areas, nor how it interacts with these areas during word production. Using direct cortical surface recordings in neurosurgical patients, we studied the evolution of activity in cortical neuronal populations, as well as the Granger causal interactions between them. We found that, during the cued production of words, a temporal cascade of neural activity proceeds from sensory representations of words in temporal cortex to their corresponding articulatory gestures in motor cortex. Broca’s area mediates this cascade through reciprocal interactions with temporal and frontal motor regions. Contrary to classic notions of the role of Broca’s area in speech, while motor cortex is activated during spoken responses, Broca’s area is surprisingly silent. Moreover, when novel strings of articulatory gestures must be produced in response to nonword stimuli, neural activity is enhanced in Broca’s area, but not in motor cortex. These unique data provide evidence that Broca’s area coordinates the transformation of information across large-scale cortical networks involved in spoken word production. In this role, Broca’s area formulates an appropriate articulatory code to be implemented by motor cortex.}}

@article{DeWitt_Rauschecker_2013,
	author = {DeWitt, Iain and Rauschecker, Josef P},
	title = {Wernicke's area revisited: parallel streams and word processing.},
	journal = {Brain and Language},
	year = {2013},
	volume = {127},
	issue = {2},
	pages = {181-191},
	doi = {10.1016/j.bandl.2013.09.014},
	issn = {0093-934X},
	pmid = {24404576},
	pmcid = {PMC4098851},
	language = {eng},
	keywords = {Animals, Humans, Speech Perception/physiology, Temporal Lobe/physiology},
	note = {Copyright © 2012 Elsevier Inc. All rights reserved.},
	PMID = {24404576},
	NLMID = {7506220},
	grant_numbers = {1RC1DC010720/DC/NIDCD NIH HHS/United States, RC1 DC010720/DC/NIDCD NIH HHS/United States, 2R56NS052494/NS/NINDS NIH HHS/United States, R01 DC003489/DC/NIDCD NIH HHS/United States, R56 NS052494/NS/NINDS NIH HHS/United States, R01 NS052494/NS/NINDS NIH HHS/United States},
	date_added = {2022-10-21},
	date_revised = {2014-11-01},
	keywords_plus = {Review, Research Support, N.I.H., Extramural, Research Support, Non-U.S. Gov't, Research Support, U.S. Gov't, Non-P.H.S.},
	pmc-release = {2014-11-01},
	doi_release = {2014-01-10},
	pubmed = {24404576},
	medline = {24404576},
	medline_date = {2014-07-30},
	pubmed_date = {2014-01-10},
	pmc_date = {2014-11-01},
	pubstatus = {ppublish},
	language = {eng},
	abstract = {Auditory word-form recognition was originally proposed by Wernicke to occur within left superior temporal gyrus (STG), later further specified to be in posterior STG. To account for clinical observations (specifically paraphasia), Wernicke proposed his sensory speech center was also essential for correcting output from frontal speech-motor regions. Recent work, in contrast, has established a role for anterior STG, part of the auditory ventral stream, in the recognition of species-specific vocalizations in nonhuman primates and word-form recognition in humans. Recent work also suggests monitoring self-produced speech and motor control are associated with posterior STG, part of the auditory dorsal stream. Working without quantitative methods or evidence of sensory cortex' hierarchical organization, Wernicke co-localized functions that today appear dissociable. "Wernicke's area" thus may be better construed as two cortical modules, an auditory word-form area (AWFA) in the auditory ventral stream and an "inner speech area" in the auditory dorsal stream.},
	address = {Netherlands},
	issn = {1090-2155 (Electronic), 0093-934X (Print), 0093-934X (Linking)},
}


@misc{bengio2014representation,
	title={Representation Learning: A Review and New Perspectives}, 
	author={Yoshua Bengio and Aaron Courville and Pascal Vincent},
	year={2014},
	eprint={1206.5538},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@INPROCEEDINGS{9098189,
  author={Fang, Zhou and Yao, Liuye and Qian, Zhiyu},
  booktitle={2019 International Conference on Medical Imaging Physics and Engineering (ICMIPE)}, 
  title={Research on Multi-parameter Visualization Technology of Brain Function Based on EEG}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ICMIPE47306.2019.9098189}
}
  
@inproceedings{10.1117/12.2255697,
	author = {Germ{\'a}n A. Pressel Coretto and Iv{\'a}n E. Gareis and H. Leonardo Rufiner},
	title = {Open access database of EEG signals recorded during imagined speech},
	volume = {10160},
	booktitle = {12th International Symposium on Medical Information Processing and Analysis},
	editor = {Eduardo Romero and Natasha Lepore and Jorge Brieva and Jorge Brieva and Ignacio Larrabide and },
	organization = {International Society for Optics and Photonics},
	publisher = {SPIE},
	pages = {1016002},
	keywords = {Imagined Speech, EEG, Database, Covert Speech, Classification},
	year = {2017},
	doi = {10.1117/12.2255697},
	URL = {https://doi.org/10.1117/12.2255697}
}

@article{ramos_lima_rodrigues_magalhães_rodrigues_destro-filho_2020,  
	title={Analysis of electroencephalography brain rhythms in the reading process},  
	volume={18},  
	ISSN={1679-4508},  
	url={https://doi.org/10.31744/einstein_journal/2020AO5442},  
	DOI={10.31744/einstein_journal/2020AO5442},  
	journal={einstein (São Paulo)},  
	publisher={Instituto Israelita de Ensino e Pesquisa Albert Einstein},  
	author={Ramos, Camila Davi and Lima, Izabella Nonato Oliveira and Rodrigues, Amanda Luiza and Magalhães, Kaliny Alice Carvalho de Oliveira and Rodrigues, Aurélia Aparecida de Araújo and Destro-Filho, João-Batista},  
	year={2020},  
	pages={eAO5442} 
}

@ARTICLE{6296526,
	author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
	journal={IEEE Signal Processing Magazine}, 
	title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
	year={2012},
	volume={29},
	number={6},
	pages={82-97},
	doi={10.1109/MSP.2012.2205597}
}

@INPROCEEDINGS{9015236,
  author={Mittal, Harsh and Sharma, Abhishek and Perumal, Thinagaran},
  booktitle={2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)}, 
  title={FPGA Implementation of Handwritten Number Recognition using Artificial Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={1010-1011},
  doi={10.1109/GCCE46687.2019.9015236}
}

@article{CHINTA2023101477,
	title = {EEG-dependent automatic speech recognition using deep residual encoder based VGG net CNN},
	journal = {Computer Speech & Language},
	volume = {79},
	pages = {101477},
	year = {2023},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2022.101477},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230822001000},
	author = {Babu Chinta and Moorthi. M},
	keywords = {Speech disorders, Electroencephalography, Eigenvector crack curvature wavelet method, Hyper similarity abnormality coder, Deep residual –encoder based VGG net CNN}
}

@inproceedings{abderrazek20_interspeech,
	author={Sondes Abderrazek and Corinne Fredouille and Alain Ghio and Muriel Lalain and Christine Meunier and Virginie Woisard},
	title={Towards Interpreting Deep Learning Models to Understand Loss of Speech Intelligibility in Speech Disorders — Step 1: CNN Model-Based Phone Classification},
	year=2020,
	booktitle={Proc. Interspeech 2020},
	pages={2522--2526},
	doi={10.21437/Interspeech.2020-2239}
}

@INPROCEEDINGS{5876829,
	author={Hu, Dingyin and Li, Wei and Chen, Xi},
	booktitle={The 2011 IEEE/ICME International Conference on Complex Medical Engineering}, 
	title={Feature extraction of motor imagery EEG signals based on wavelet packet decomposition}, 
	year={2011},
	volume={},
	number={},
	pages={694-697},
	doi={10.1109/ICCME.2011.5876829}
}

@ARTICLE{9141493,
	author={Appriou, Aurelien and Cichocki, Andrzej and Lotte, Fabien},
	journal={IEEE Systems, Man, and Cybernetics Magazine}, 
	title={Modern Machine-Learning Algorithms: For Classifying Cognitive and Affective States From Electroencephalography Signals}, 
	year={2020},
	volume={6},
	number={3},
	pages={29-38},
	doi={10.1109/MSMC.2020.2968638}
}

@ARTICLE{6165246,
	author={van Erp, Jan and Lotte, Fabien and Tangermann, Michael},
	journal={Computer}, 
	title={Brain-Computer Interfaces: Beyond Medical Applications}, 
	year={2012},
	volume={45},
	number={4},
	pages={26-34},
	doi={10.1109/MC.2012.107}
}

@article{singh2021decoding,
	title={Decoding imagined speech and computer control using brain waves},
	author={Singh, Abhiram and Gumaste, Ashwin},
	journal={Journal of neuroscience methods},
	volume={358},
	pages={109196},
	year={2021},
	publisher={Elsevier}
}

@article{singh2020interpreting,
	title={Interpreting imagined speech waves with machine learning techniques},
	author={Singh, Abhiram and Gumaste, Ashwin},
	journal={arXiv preprint arXiv:2010.03360},
	year={2020}
}

@inproceedings{lee2020classification,
	title={Classification of imagined speech using siamese neural network},
	author={Lee, Dong-Yeon and Lee, Minji and Lee, Seong-Whan},
	booktitle={2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	pages={2979--2984},
	year={2020},
	organization={IEEE}
}

@INPROCEEDINGS{7178118,
	author={Zhao, Shunan and Rudzicz, Frank},
	booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
	title={Classifying phonological categories in imagined and articulated speech}, 
	year={2015},
	volume={},
	number={},
	pages={992-996},
	keywords={Electroencephalography;Presses;Tin;Phonological categories;electroencephalography;speech articulation;deep-belief networks},
	doi={10.1109/ICASSP.2015.7178118}
}


@inproceedings{cooney2019optimizing,
	title={Optimizing layers improves CNN generalization and transfer learning for imagined speech decoding from EEG},
	author={Cooney, Ciaran and Folli, Raffaella and Coyle, Damien},
	booktitle={2019 IEEE international conference on systems, man and cybernetics (SMC)},
	pages={1311--1316},
	year={2019},
	organization={IEEE}
}

@article{lee2020neural,
	title={Neural decoding of imagined speech and visual imagery as intuitive paradigms for BCI communication},
	author={Lee, Seo-Hyun and Lee, Minji and Lee, Seong-Whan},
	journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume={28},
	number={12},
	pages={2647--2659},
	year={2020},
	publisher={IEEE}
}

@inproceedings{Panachakel_2019,
	title={Decoding Imagined Speech using Wavelet Features and Deep Neural Networks},
	url={http://dx.doi.org/10.1109/INDICON47234.2019.9028925},
	DOI={10.1109/indicon47234.2019.9028925},
	booktitle={2019 IEEE 16th India Council International Conference (INDICON)},
	publisher={IEEE},
	author={Panachakel, Jerrin Thomas and Ramakrishnan, A.G. and Ananthapadmanabha, T.V.},
	year={2019},
	month=dec
}


@article{min2016vowel,
	title={Vowel imagery decoding toward silent speech BCI using extreme learning machine with electroencephalogram},
	author={Min, Beomjun and Kim, Jongin and Park, Hyeong-jun and Lee, Boreom},
	journal={BioMed research international},
	volume={2016},
	year={2016},
	publisher={Hindawi}
}

@article{ WOS:000936149000001,
	Author = {Kamble, Ashwin and Ghare, Pradnya H. and Kumar, Vinay},
	Title = {Optimized Rational Dilation Wavelet Transform for Automatic Imagined Speech Recognition},
	Journal = {IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT},
	Year = {2023},
	Volume = {72},
	DOI = {10.1109/TIM.2023.3241973},
	Article-Number = {4002210},
	ISSN = {0018-9456},
	EISSN = {1557-9662},
	Unique-ID = {WOS:000936149000001},
}

@INPROCEEDINGS{5515807,
	author={Brigham, Katharine and Kumar, B. V. K. Vijaya},
	booktitle={2010 4th International Conference on Bioinformatics and Biomedical Engineering}, 
	title={Imagined Speech Classification with EEG Signals for Silent Communication: A Preliminary Investigation into Synthetic Telepathy}, 
	year={2010},
	volume={},
	number={},
	pages={1-4},
	doi={10.1109/ICBBE.2010.5515807}
}

@inproceedings{idrees2016vowel,
	title={Vowel classification using wavelet decomposition during speech imagery},
	author={Idrees, Basil M and Farooq, Omar},
	booktitle={2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN)},
	pages={636--640},
	year={2016},
	organization={IEEE}
}

@incollection{khare2020classification,
	title={Classification of schizophrenia patients through empirical wavelet transformation using electroencephalogram signals},
	author={Khare, Smith K and Bajaj, Varun and Siuly, Siuly and Sinha, GR},
	booktitle={Modelling and Analysis of Active Biopotential Signals in Healthcare, Volume 1},
	year={2020},
	publisher={IOP Publishing}
}

@article{WU2011170,
	title = {Frequency recognition in an SSVEP-based brain computer interface using empirical mode decomposition and refined generalized zero-crossing},
	journal = {Journal of Neuroscience Methods},
	volume = {196},
	number = {1},
	pages = {170-181},
	year = {2011},
	issn = {0165-0270},
	doi = {https://doi.org/10.1016/j.jneumeth.2010.12.014},
	url = {https://www.sciencedirect.com/science/article/pii/S0165027010007028},
	author = {Chi-Hsun Wu and Hsiang-Chih Chang and Po-Lei Lee and Kuen-Shing Li and Jyun-Jie Sie and Chia-Wei Sun and Chia-Yen Yang and Po-Hung Li and Hua-Ting Deng and Kuo-Kai Shyu},
	keywords = {Brain computer interface (BCI), Empirical mode decomposition (EMD), Steady-state visual evoked potential (SSVEP), Refined generalized zero-crossing (rGZC)},
}

@ARTICLE{6400235,
	author={Shahidi Zandi, Ali and Tafreshi, Reza and Javidan, Manouchehr and Dumont, Guy A.},
	journal={IEEE Transactions on Biomedical Engineering}, 
	title={Predicting Epileptic Seizures in Scalp EEG Based on a Variational Bayesian Gaussian Mixture Model of Zero-Crossing Intervals}, 
	year={2013},
	volume={60},
	number={5},
	pages={1401-1413},
	doi={10.1109/TBME.2012.2237399}
}

@ARTICLE{6400235, 
	author={Shahidi Zandi, Ali and Tafreshi, Reza and Javidan, Manouchehr and Dumont, Guy A.}, 
	journal={IEEE Transactions on Biomedical Engineering}, 
	title={Predicting Epileptic Seizures in Scalp EEG Based on a Variational Bayesian Gaussian Mixture Model of Zero-Crossing Intervals}, 
	year={2013}, 
	volume={60}, 
	number={5}, 
	pages={1401-1413}, 
	doi={10.1109/TBME.2012.2237399}
} 

@article{GUIDO2016248,
	title = {ZCR-aided neurocomputing: A study with applications},
	journal = {Knowledge-Based Systems},
	volume = {105},
	pages = {248-269},
	year = {2016},
	issn = {0950-7051},
	doi = {https://doi.org/10.1016/j.knosys.2016.05.011},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705116301009},
	author = {Rodrigo Capobianco Guido},
	keywords = {Zero-crossing rates (ZCRs), Pattern recognition and knowledge-based systems (PRKbS), Feature extraction (FE), Speech segmentation, Image border extraction, Biomedical signal analysis},
}

@book{beigi2011fundamentals,
	title={Fundamentals of Speaker Recognition},
	author={Beigi, H.},
	isbn={9780387775913},
	lccn={2011941119},
	year={2011},
	publisher={Springer US}
}

@book{haykinredes,
	title={Redes Neurais: Princ{\'\i}pios e Pr{\'a}tica},
	author={Simon Haykin},
	isbn={9788577800865},
	publisher={Bookman Editora},
	year={2001}
}

@book{rashid2016make,
	title={Make Your Own Neural Network: A Gentle Journey Through the Mathematics of Neural Networks, and Making Your Own Using the Python Computer Language},
	author={Rashid, T.},
	isbn={9781530826605},
	year={2016},
	publisher={CreateSpace Independent Publishing}
}

@article{DBLP:journals/corr/HeZRS15,
	author    = {Kaiming He and	Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	eprinttype = {arXiv},
	eprint    = {1512.03385},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{UYCNWV,
	author = {Ankit Paliwal},
	title = {Understand your convolution network with visualizations},
	date = {2022-08-27},
	url = {https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b},
}

@online{CNNC,
	author = {Afshine Amidi and Shervine Amidi},
	title = {Convolutional Neural Networks cheatsheet},
	date = {2022-08-27},
	url = {https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks},
}
@mastersthesis{furlan2021caracterizacao,
	author = {André, Furlan},
	title = {Caracterização de voice spoofing para fins de verificação de locutores com base na Transformada Wavelet e na Análise Paraconsistente de Características},
	year = {2021},
	school = {Universidade Estadual Paulista - campus de São José do Rio Preto-SP},
	address = {São José do Rio Preto, Brazil},
	type = {Dissertação de Mestrado},
	note = {Orientador: Prof Dr Rodrigo Capobianco Guido}
}

@book{beigi2011speaker,
	title={Speaker recognition},
	author={Beigi, Homayoon and Beigi, Homayoon},
	year={2011},
	publisher={Springer}
}

@book{neustein2012forensic,
	title={Forensic speaker recognition},
	author={Neustein, Amy and Patil, Hemant A},
	volume={1},
	year={2012},
	publisher={Springer}
}

@article{hansen2015speaker,
	title={Speaker recognition by machines and humans: A tutorial review},
	author={Hansen, John HL and Hasan, Taufiq},
	journal={IEEE Signal processing magazine},
	volume={32},
	number={6},
	pages={74--99},
	year={2015},
	publisher={IEEE}
}

@article{wang2022racp,
	title={RACP: A network with attention corrected prototype for few-shot speaker recognition using indefinite distance metric},
	author={Wang, Xingmei and Meng, Jiaxiang and Wen, Bin and Xue, Fuzhao},
	journal={Neurocomputing},
	volume={490},
	pages={283--294},
	year={2022},
	publisher={Elsevier}
}

@article{lee2020two,
	title={Two decades into speaker recognition evaluation-are we there yet?},
	author={Lee, Kong Aik and Sadjadi, Seyed Omid and Li, Haizhou and Reynolds, Douglas A},
	journal={Comput. Speech Lang.},
	volume={61},
	pages={101058},
	year={2020}
}

@article{chaiani2022voice,
	title={Voice disorder classification using speech enhancement and deep learning models},
	author={Chaiani, Mounira and Selouani, Sid Ahmed and Boudraa, Malika and Yakoub, Mohammed Sidi},
	journal={Biocybernetics and Biomedical Engineering},
	volume={42},
	number={2},
	pages={463--480},
	year={2022},
	publisher={Elsevier}
}

@article{fujimura2022classification,
	title={Classification of voice disorders using a one-dimensional convolutional neural network},
	author={Fujimura, Shintaro and Kojima, Tsuyoshi and Okanoue, Yusuke and Shoji, Kazuhiko and Inoue, Masato and Omori, Koichi and Hori, Ryusuke},
	journal={Journal of Voice},
	volume={36},
	number={1},
	pages={15--20},
	year={2022},
	publisher={Elsevier}
}

@book{goodfellow2016deep,
	title={Deep learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year={2016},
	publisher={MIT press}
}

@article{mittal2021deep,
	title={Deep learning approach for voice pathology detection and classification},
	author={Mittal, Vikas and Sharma, RK},
	journal={International Journal of Healthcare Information Systems and Informatics (IJHISI)},
	volume={16},
	number={4},
	pages={1--30},
	year={2021},
	publisher={IGI Global}
}

@inproceedings{miliaresi2021combining,
	title={Combining acoustic features and medical data in deep learning networks for voice pathology classification},
	author={Miliaresi, Ioanna and Poutos, Kyriakos and Pikrakis, Aggelos},
	booktitle={2020 28th European Signal Processing Conference (EUSIPCO)},
	pages={1190--1194},
	year={2021},
	organization={IEEE}
}


@inproceedings{fonseca2017linear,
	title={Linear prediction and discrete wavelet transform to identify pathology in voice signals},
	author={Fonseca, Everthon Silva and Pereira, Denis C{\'e}sar Mosconi and Maschi, Lu{\'\i}s Fernando Castilho and Guido, Rodrigo Capobianco and Paulo, Katia Cristina Silva},
	booktitle={2017 Signal Processing Symposium (SPSympo)},
	pages={1--4},
	year={2017},
	organization={Ieee}
}

@incollection{le2005voz,
	title={A voz: patologia vocal de origem funcional},
	author={Le Huche, Francois and Allali, Andr{\'e}},
	booktitle={A voz: patologia vocal de origem funcional},
	pages={187--187},
	year={2005}
}

@incollection{le2005voz2,
	title={A voz: patologia vocal de origem org{\^a}nica},
	author={Le Huche, Fran{\c{c}}ois and Allali, Andr{\'e}},
	booktitle={A voz: patologia vocal de origem org{\^a}nica},
	pages={154--154},
	year={2005}
}

@article{gupta2021residual,
	title={Residual Neural Network precisely quantifies dysarthria severity-level based on short-duration speech segments},
	author={Gupta, Siddhant and Patil, Ankur T and Purohit, Mirali and Parmar, Mihir and Patel, Maitreya and Patil, Hemant A and Guido, Rodrigo Capobianco},
	journal={Neural Networks},
	volume={139},
	pages={105--117},
	year={2021},
	publisher={Elsevier}
}

@article{moctezuma2019subjects,
	title={Subjects identification using EEG-recorded imagined speech},
	author={Moctezuma, Luis Alfredo and Torres-Garc{\'\i}a, Alejandro A and Villase{\~n}or-Pineda, Luis and Carrillo, Maya},
	journal={Expert Systems with Applications},
	volume={118},
	pages={201--208},
	year={2019},
	publisher={Elsevier}
}

@inproceedings{rusnac2021eeg,
	title={EEG Preprocessing Methods for BCI Imagined Speech Signals},
	author={Rusnac, Ana-Luiza and Grigore, Ovidiu},
	booktitle={2021 International Conference on e-Health and Bioengineering (EHB)},
	pages={1--4},
	year={2021},
	organization={IEEE}
}


@inproceedings{brigham2010imagined,
	title={Imagined speech classification with EEG signals for silent communication: a preliminary investigation into synthetic telepathy},
	author={Brigham, Katharine and Kumar, BVK Vijaya},
	booktitle={2010 4th International Conference on Bioinformatics and Biomedical Engineering},
	pages={1--4},
	year={2010},
	organization={IEEE}
}


@article{pawar2022wavelet,
	title={Wavelet-based imagined speech classification using electroencephalography},
	author={Pawar, Dipti and Dhage, Sudhir},
	journal={International Journal of Biomedical Engineering and Technology},
	volume={38},
	number={3},
	pages={215--224},
	year={2022},
	publisher={Inderscience Publishers (IEL)}
}

@article{ParkHyeong-jun2023Mcoi,
	pages = {1186594-1186594},
	publisher = {Frontiers Media S.A},
	title = {Multiclass classification of imagined speech EEG using noise-assisted multivariate empirical mode decomposition and multireceptive field convolutional neural network},
	volume = {17},
	year = {2023},
	abstract = {Introduction In this study, we classified electroencephalography (EEG) data of imagined speech using signal decomposition and multireceptive convolutional neural network. The imagined speech EEG with five vowels /a/, /e/, /i/, /o/, and /u/, and mute (rest) sounds were obtained from ten study participants. Materials and methods First, two different signal decomposition methods were applied for comparison: noise-assisted multivariate empirical mode decomposition and wavelet packet decomposition. Six statistical features were calculated from the decomposed eight sub-frequency bands EEG. Next, all features obtained from each channel of the trial were vectorized and used as the input vector of classifiers. Lastly, EEG was classified using multireceptive field convolutional neural network and several other classifiers for comparison. Results We achieved an average classification rate of 73.09 and up to 80.41% in a multiclass (six classes) setup (Chance: 16.67%). In comparison with various other classifiers, significant improvements for other classifiers were achieved ( p -value < 0.05). From the frequency sub-band analysis, high-frequency band regions and the lowest-frequency band region contain more information about imagined vowel EEG data. The misclassification and classification rate of each vowel imaginary EEG was analyzed through a confusion matrix. Discussion Imagined speech EEG can be classified successfully using the proposed signal decomposition method and a convolutional neural network. The proposed classification method for imagined speech EEG can contribute to developing a practical imagined speech-based brain-computer interfaces system.},
	author = {Park, Hyeong-jun and Lee, Boreom},
	copyright = {Copyright © 2023 Park and Lee. 2023 Park and Lee},
	issn = {1662-5161},
	journal = {Frontiers in human neuroscience},
	keywords = {brain-computer interfaces ; imagined speech EEG ; multiclass classification ; multireceptive field convolutional neural network ; Neuroscience ; noise-assisted empirical mode decomposition},
	language = {eng},
}


@article{cooney2021bimodal,
	title={A bimodal deep learning architecture for EEG-fNIRS decoding of overt and imagined speech},
	author={Cooney, Ciaran and Folli, Raffaella and Coyle, Damien},
	journal={IEEE Transactions on Biomedical Engineering},
	volume={69},
	number={6},
	pages={1983--1994},
	year={2021},
	publisher={IEEE}
}

@article{lopez2022state,
	title={A State-of-the-Art Review of EEG-Based Imagined Speech Decoding},
	author={Lopez-Bernal, Diego and Balderas, David and Ponce, Pedro and Molina, Arturo},
	journal={Frontiers in Human Neuroscience},
	volume={16},
	year={2022},
	publisher={Frontiers Media SA}
}

@article{bakhshali2022investigating,
	title={Investigating the neural correlates of imagined speech: An EEG-based connectivity analysis},
	author={Bakhshali, Mohamad Amin and Khademi, Morteza and Ebrahimi-Moghadam, Abbas},
	journal={Digital Signal Processing},
	volume={123},
	pages={103435},
	year={2022},
	publisher={Elsevier}
}

@article{lee2021decoding,
	title={Decoding imagined speech based on deep metric learning for intuitive BCI communication},
	author={Lee, Dong-Yeon and Lee, Minji and Lee, Seong-Whan},
	journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume={29},
	pages={1363--1374},
	year={2021},
	publisher={IEEE}
}


@article{tamm2020classification,
	title={Classification of vowels from imagined speech with convolutional neural networks},
	author={Tamm, Markus-Oliver and Muhammad, Yar and Muhammad, Naveed},
	journal={Computers},
	volume={9},
	number={2},
	pages={46},
	year={2020},
	publisher={MDPI}
}

@inproceedings{moctezuma2018eeg,
	title={EEG-based Subjects Identification based on Biometrics of Imagined Speech using EMD},
	author={Moctezuma, Luis Alfredo and Molinas, Marta},
	booktitle={Brain Informatics: International Conference, BI 2018, Arlington, TX, USA, December 7--9, 2018, Proceedings 11},
	pages={458--467},
	year={2018},
	organization={Springer}
}


@inproceedings{jayarathne2016brainid,
	title={BrainID: Development of an EEG-based biometric authentication system},
	author={Jayarathne, Isuru and Cohen, Michael and Amarakeerthi, Senaka},
	booktitle={2016 IEEE 7th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
	pages={1--6},
	year={2016},
	organization={IEEE}
}

@inproceedings{jayarathne2017survey,
	title={Survey of EEG-based biometric authentication},
	author={Jayarathne, Isuru and Cohen, Michael and Amarakeerthi, Senaka},
	booktitle={2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST)},
	pages={324--329},
	year={2017},
	organization={IEEE}
}

@article{del2014electroencephalogram,
	title={Electroencephalogram subject identification: A review},
	author={Del Pozo-Banos, Marcos and Alonso, Jes{\'u}s B and Ticay-Rivas, Jaime R and Travieso, Carlos M},
	journal={Expert Systems with Applications},
	volume={41},
	number={15},
	pages={6537--6554},
	year={2014},
	publisher={Elsevier}
}


@article{ruiz2016cerebre,
	title={CEREBRE: A novel method for very high accuracy event-related potential biometric identification},
	author={Ruiz-Blondet, Maria V and Jin, Zhanpeng and Laszlo, Sarah},
	journal={IEEE Transactions on Information Forensics and Security},
	volume={11},
	number={7},
	pages={1618--1629},
	year={2016},
	publisher={IEEE}
}


@article{ WOS:000591530700001,
	Author = {Mini, P. P. and Thomas, Tessamma and Gopikakumari, R.},
	Title = {Wavelet feature selection of audio and imagined/vocalized EEG signals
	for ANN based multimodal ASR system},
	Journal = {BIOMEDICAL SIGNAL PROCESSING AND CONTROL},
	Year = {2021},
	Volume = {63},
	Month = {JAN},
	DOI = {10.1016/j.bspc.2020.102218},
	Article-Number = {102218},
	ISSN = {1746-8094},
	EISSN = {1746-8108},
	Unique-ID = {WOS:000591530700001},
}

@article{tharwat2020classification,
	title={Classification assessment methods},
	author={Tharwat, Alaa},
	journal={Applied Computing and Informatics},
	volume={17},
	number={1},
	pages={168--192},
	year={2020},
	publisher={Emerald Publishing Limited}
}

@article{ WOS:000857544900001,
	Author = {Shah, Uzair and Alzubaidi, Mahmood and Mohsen, Farida and Abd-Alrazaq,
	Alaa and Alam, Tanvir and Househ, Mowafa},
	Title = {The Role of Artificial Intelligence in Decoding Speech from EEG Signals:
	A Scoping Review},
	Journal = {SENSORS},
	Year = {2022},
	Volume = {22},
	Number = {18},
	Month = {SEP},
	DOI = {10.3390/s22186975},
	Article-Number = {6975},
	EISSN = {1424-8220},
	ResearcherID-Numbers = {Househ, Mowafa/GPX-8430-2022
	},
	ORCID-Numbers = {Alam, Tanvir/0000-0001-7033-3693},
	Unique-ID = {WOS:000857544900001},
}


@article{ WOS:000614122200021,
	Author = {Hernandez-Del-Toro, Tonatiuh and Reyes-Garcia, Carlos A. and
	Villasenor-Pineda, Luis},
	Title = {Toward asynchronous EEG-based BCI: Detecting imagined words segments in
	continuous EEG signals},
	Journal = {BIOMEDICAL SIGNAL PROCESSING AND CONTROL},
	Year = {2021},
	Volume = {65},
	Month = {MAR},
	DOI = {10.1016/j.bspc.2020.102351},
	Article-Number = {102351},
	ISSN = {1746-8094},
	EISSN = {1746-8108},
	ORCID-Numbers = {Hernandez-Del-Toro, Tonatiuh/0000-0002-5186-6168},
	Unique-ID = {WOS:000614122200021},
}


@inproceedings{jin21_interspeech,
	author={Zengrui Jin and Mengzhe Geng and Xurong Xie and Jianwei Yu and Shansong Liu and Xunying Liu and Helen Meng},
	title={{Adversarial Data Augmentation for Disordered Speech Recognition}},
	year=2021,
	booktitle={Proc. Interspeech 2021},
	pages={4803--4807},
	doi={10.21437/Interspeech.2021-168}
}

@inproceedings{ WOS:000841879504172,
	Author = {Green, Jordan R. and MacDonald, Robert L. and Jiang, Pan-Pan and
	Cattiau, Julie and Heywood, Rus and Cave, Richard and Seaver, Katie and
	Ladewig, Marilyn A. and Tobin, Jimmy and Brenner, Michael P. and Nelson,
	Philip C. and Tomanek, Katrin},
	Book-Group-Author = {Int Speech Commun Assoc},
	Title = {Automatic Speech Recognition of Disordered Speech: Personalized models
	outperforming human listeners on short phrases},
	Booktitle = {INTERSPEECH 2021},
	Series = {Interspeech},
	Year = {2021},
	Pages = {4778-4782},
	Note = {Interspeech Conference, Brno, CZECH REPUBLIC, AUG 30-SEP 03, 2021},
	DOI = {10.21437/Interspeech.2021-1384},
	ISSN = {2308-457X},
	ResearcherID-Numbers = {Green, Jordan R/M-6113-2013},
	ORCID-Numbers = {Green, Jordan R/0000-0002-1464-1373},
	Unique-ID = {WOS:000841879504172},
}


@inproceedings{ WOS:000652176200030,
	Author = {Chen, Ko-Chiang and Han, Ji-Yan and Jhang, Sin-Hua and Lai, Ying-Hui},
	Editor = {Lin, KP and Magjarevic, R and DeCarvalho, P},
	Title = {A Study of Speech Phase in Dysarthria Voice Conversion System},
	Booktitle = {FUTURE TRENDS IN BIOMEDICAL AND HEALTH INFORMATICS AND CYBERSECURITY IN
	MEDICAL DEVICES, ICBHI 2019},
	Series = {IFMBE Proceedings},
	Year = {2020},
	Volume = {74},
	Pages = {219-226},
	Note = {International Conference on Biomedical and Health Informatics (ICBHI),
	Taipei, TAIWAN, APR 17-20, 2019},
	Organization = {Chung Yuan Christian Univ, Technol Translat Ctr Med Device; IFMBE WG
	Hlth Informat \& EHealth; Minist Sci \& Technol; Natl Hlth Res Inst},
	DOI = {10.1007/978-3-030-30636-6\_31},
	ISSN = {1680-0737},
	ISBN = {978-3-030-30636-6; 978-3-030-30635-9},
	Unique-ID = {WOS:000652176200030},
}

@article{salim2023automatic,
	title={Automatic speaker verification system for dysarthric speakers using prosodic features and out-of-domain data augmentation},
	author={Salim, Shinimol and Shahnawazuddin, Syed and Ahmad, Waquar},
	journal={Applied Acoustics},
	volume={210},
	pages={109412},
	year={2023},
	publisher={Elsevier}
}

@INPROCEEDINGS{8396208,
	author={A. {Awais} and S. {Kun} and Y. {Yu} and S. {Hayat} and A. {Ahmed} and T. {Tu}},
	booktitle={2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)},
	title={Speaker recognition using mel frequency cepstral coefficient and locality sensitive hashing},
	year={2018},
	volume={},
	number={},
	pages={271-276},
	doi={10.1109/ICAIBD.2018.8396208},
	ISSN={null},
	month={May}
}

@online{TIMIT2018,
	author = {Linguistic Data Consortium},
	title = {TIMIT Acoustic-Phonetic Continuous Speech Corpus},
	date = {2018},
	year={2018},
	url = {https://catalog.ldc.upenn.edu/byyear#2018},
}

@book{haykin2011sistemas,
	title={Sistemas de Comunica{\c{c}}{\~a}o-5},
	author={Haykin, Simon and Moher, Michael},
	year={2011},
	publisher={Bookman Editora}
}

@article{kremer2014eficiencia,
	title={A efici{\^e}ncia do disfarce em vozes femininas: uma an{\'a}lise da frequ{\^e}ncia fundamental},
	author={Kremer, Robinson Luis and GOMES, ML d C},
	journal={ReVEL},
	volume={12},
	pages={23},
	year={2014}
}

@article{WERTZNER2005,
	title={An{\'a}lise da freq{\"u}{\^e}ncia fundamental, jitter, shimmer e intensidade vocal em crian{\c{c}}as com transtorno fonol{\'o}gico},
	author={Wertzner, Hayd{\'e}e F and Schreiber, Solange and Amaro, Luciana},
	journal={Revista Brasileira de Otorrinolaringologia},
	volume={71},
	pages={582--588},
	year={2005},
	publisher={SciELO Brasil}
}

@article{freitas2013avaliaccao,
	title={Avalia{\c{c}}{\~a}o ac{\'u}stica e {\'a}udio percetiva na caracteriza{\c{c}}{\~a}o da voz humana},
	author={Freitas, Susana},
	year={2013},
	publisher={Faculdade de Engenharia da Universidade do Porto}
}

@article{valencca2014analise,
	title={An{\'a}lise ac{\'u}stica dos formantes em indiv{\'\i}duos com defici{\^e}ncia isolada do horm{\^o}nio do crescimento},
	author={Valen{\c{c}}a, Eug{\^e}nia Herm{\'\i}nia Oliveira and others},
	year={2014},
	publisher={Universidade Federal de Sergipe}
}

@article{doi:10.1121-1.1908630,
	author = {Zwicker, E.},
	title = {Subdivision of the Audible Frequency Range into Critical Bands (Frequenzgruppen)},
	journal = {The Journal of the Acoustical Society of America},
	volume = {33},
	number = {2},
	pages = {248-248},
	year = {1961},
	doi = {10.1121/1.1908630},
}

@book{beranek1949acoustic,
	title={Acoustic Measurements},
	author={Beranek, Leo L.},
	year={1949},
	organization={United States. Navy Dept. Office of Naval Research and United States. Office of Naval Research},
	lccn={49048299},
	publisher={J. Wiley}
}

@book{bosi2002introduction,
	title={Introduction to digital audio coding and standards},
	author={Bosi, Marina and Goldberg, Richard E},
	volume={721},
	year={2002},
	publisher={Springer Science \& Business Media}
}

@article{shirani2008data,
	title={Data compression: The complete reference (by d. salomon; 2007)[book review]},
	author={Shirani, Shahram},
	journal={IEEE Signal Processing Magazine},
	volume={25},
	number={2},
	pages={147--149},
	year={2008},
	publisher={IEEE}
}

@book{salomon2007data,
	title={Data Compression: The Complete Reference},
	author={Salomon, D. and Motta, G. and Bryant, D.},
	isbn={9781846286032},
	lccn={2006931789},
	series={Molecular biology intelligence unit},
	url={https://books.google.com.br/books?id=ujnQogzx\_2EC},
	year={2007},
	publisher={Springer London}
}

@book{jolliffe2006principal,
	title={Principal Component Analysis},
	author={Jolliffe, I.T.},
	isbn={9780387954424},
	lccn={2002019560},
	series={Springer Series in Statistics},
	url={https://books.google.com.br/books?id=\_olByCrhjwIC},
	year={2002},
	publisher={Springer}
}

@ARTICLE{Rod5254905,
	author={P. S. Addison and J. Walker and R. C. Guido},
	journal={IEEE Engineering in Medicine and Biology Magazine},
	title={Time--frequency analysis of biosignals},
	year={2009},
	volume={28},
	number={5},
	pages={14-29},
	doi={10.1109/MEMB.2009.934244},
	month={Sep},
}

@misc{robi2003,
	title={The wavelet tutorial},
	author={Polikar, Robi and others},
	year={1996}
}

@book{daubechies1992ten,
	title={Ten Lectures on Wavelets},
	author={Daubechies, I.},
	series={CBMS-NSF Regional Conference Series in Applied Mathematics},
	year={1992},
	publisher={Society for Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104)}
}

@article{butterworth1930,
	author = {S. Butterworth},
	title = {On the theory of filters amplifiers},
	date = {10/1930},
	year = {1930},
}

@book{bianchi2007electronic,
	title={Electronic Filter Simulation \& Design},
	author={Bianchi, G.},
	isbn={9780071712620},
	lccn={2007016736},
	year={2007},
	publisher={McGraw-Hill Education}
}

@book{Jensen_2001,
	doi = {10.1007/978-3-642-56702-5},
	year = 2001,
	publisher = {Springer Berlin Heidelberg},
	author = {Arne Jensen and Anders la Cour-Harbo},
	title = {Ripples in Mathematics}
}


@ARTICLE{8588433,
	author={R. C. Guido},
	journal={IEEE Signal Processing Magazine},
	title={Paraconsistent Feature Engineering [Lecture Notes]},
	year={2019},
	volume={36},
	number={1},
	pages={154-158},
	doi={10.1109/MSP.2018.2874549},
	month={Jan},
}

@book{da1998elementos,
	title={Elementos de teoria paraconsistente de conjuntos},
	author={da Costa, N.C.A. and B{\'e}ziau, J.Y. and Bueno, O.},
	series={Cole{\c{c}}{\~a}o CLE},
	url={https://books.google.com.br/books?id=MGCKGwAACAAJ},
	year={1998},
	publisher={Centro de L{\'o}gica, Epistemologia e Hist{\'o}ria da Ci{\^e}ncia, UNICAMP}
}

@article{COSTA2000,
	title = {Paraconsistência em informática e inteligência artificial},
	journal = {Estudos Avançados},
	author={Costa, Newton C.A. da AND Abe, Jair Minoro},
	volume = {14},
	year = {2000},
	month = {08},
	pages = {161 - 174},
	publisher = {scielo},
}

@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@book{kasabov2018time,
	title={Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence},
	author={Kasabov, N.K.},
	isbn={9783662577158},
	series={Springer Series on Bio- and Neurosystems},
	url={https://books.google.com.br/books?id=YQRrDwAAQBAJ},
	year={2018},
	publisher={Springer Berlin Heidelberg}
}

@inproceedings{zhao2015classifying,
	title={Classifying phonological categories in imagined and articulated speech},
	author={Zhao, Shunan and Rudzicz, Frank},
	booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={992--996},
	year={2015},
	organization={IEEE}
}
@article{dsp4,
title = {Optimization of data-driven filterbank for automatic speaker verification},
journal = {Digital Signal Processing},
volume = {104},
pages = {102795},
year = {2020},
doi = {https://doi.org/10.1016/j.dsp.2020.102795},
author = {Susanta Sarangi and Md Sahidullah and Goutam Saha},
}

@article{dsp3,
title = {Low frequency frame-wise normalization over constant-Q transform for playback speech detection},
journal = {Digital Signal Processing},
volume = {89},
pages = {30-39},
year = {2019},
doi = {https://doi.org/10.1016/j.dsp.2019.02.018},
author = {Jichen Yang and Rohan Kumar Das},
}


@article{dsp2,
title = {Quality measures for speaker verification with short utterances},
journal = {Digital Signal Processing},
volume = {88},
pages = {66-79},
year = {2019},
doi = {https://doi.org/10.1016/j.dsp.2019.01.023},
author = {Arnab Poddar and Md Sahidullah and Goutam Saha},
}

@article{dsp1,
title = {Data selection for i-vector based automatic speaker verification anti-spoofing},
journal = {Digital Signal Processing},
volume = {72},
pages = {171-180},
year = {2018},
doi = {https://doi.org/10.1016/j.dsp.2017.10.010},
author = {Cemal Hanilçi},
}

@Book{bishop:2006:PRML,
  author = 	 "Christopher M. Bishop",
  title = 	 "Pattern Recognition and Machine Learning",
  publisher = "Springer",
  year = 	 "2006"
}

@article{vs1,
author={- - -},
title = {HSBC reports high trust levels in biometric tech as twins spoof its voice ID system},
journal = {Biometric Technology Today},
number = {6},
pages = {12},
year = {2017},
}

@article{vs2,
title = {Voice spoofing detection corpus for single and multi-order audio replays},
journal = {Computer Speech \& Language},
volume = {65},
pages = {101132},
year = {2021},
author = {Roland Baumann and Khalid Mahmood Malik and Ali Javed and Andersen Ball and Brandon Kujawa and Hafiz Malik}
}

@ARTICLE{dwt1,
  author={Guido, Rodrigo Capobianco},
  journal={IEEE Signal Processing Magazine}, 
  title={Effectively Interpreting Discrete Wavelet Transformed Signals [Lecture Notes]}, 
  year={2017},
  volume={34},
  number={3},
  pages={89-100}
 }

@article{guido2,
author = {Rodrigo Capobianco Guido},
title = {Nearly symmetric orthogonal wavelets for time-frequency-shape joint analysis: Introducing the discrete shapelet transform’s third generation (DST-III) for nonlinear signal analysis},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {97},
pages = {105685},
year = {2021}
}

@article{guido3,
author = {Rodrigo Capobianco Guido and Jan Frans Willem Slaets and Roland Köberle and Lírio Onofre Batista Almeida and José Carlos Pereira},
title = {A new technique to construct a wavelet transform matching a specified signal with applications to digital, real time, spike, and overlap pattern recognition},
journal = {Digital Signal Processing},
volume = {16},
number = {1},
pages = {24-44},
year = {2006}
}

@article{guido4,
title = {Fusing time, frequency and shape-related information: Introduction to the Discrete Shapelet Transform’s second generation (DST-II)},
journal = {Information Fusion},
volume = {41},
pages = {9-15},
year = {2018},
author = {Rodrigo Capobianco Guido}
}

@article{tut_se,
title = {A tutorial on signal energy and its applications},
journal = {Neurocomputing},
volume = {179},
pages = {264-282},
year = {2016},
author = {Rodrigo Capobianco Guido}
}

@article{bossi,
title = {Speech emotion recognition using cepstral features extracted with novel triangular filter banks based on bark and ERB frequency scales},
journal = {Digital Signal Processing},
volume = {104},
pages = {102763},
year = {2020},
author = {Sugan Nagarajan and Satya Sai Srinivas Nettimi and Lakshmi Sutha Kumar and Malaya Kumar Nath and Aniruddha Kanhe} 
}

@article{bossi2,
title = {Improved speech emotion recognition with Mel frequency magnitude coefficient},
journal = {Applied Acoustics},
volume = {179},
pages = {108046},
year = {2021},
author = {J. Ancilin and A. Milton}
}

@ARTICLE{guidodwt1,
  author={Guido, Rodrigo Capobianco},
  journal={IEEE Signal Processing Magazine}, 
  title={Effectively Interpreting Discrete Wavelet Transformed Signals [Lecture Notes]}, 
  year={2017},
  volume={34},
  number={3},
  pages={89-100}
}

@ARTICLE{np1,
  author={Yoon, Sung-Hyun and Koh, Min-Sung and Park, Jae-Han and Yu, Ha-Jin},
  journal={IEEE Access}, 
  title={A New Replay Attack Against Automatic Speaker Verification Systems}, 
  year={2020},
  volume={8},
  number={},
  pages={36080-36088}
}

@inproceedings{np2,
  author={Saranya M S and Hema Murthy},
  title={Decision-level Feature Switching as a Paradigm for Replay Attack Detection},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={686--690}
}

@book{poole2014linear,
	title={Linear Algebra: A Modern Introduction},
	author={Poole, D.},
	year={2014},
	publisher={Cengage Learning}
}

@book{ghazali2018recent,
	title={Recent Advances on Soft Computing and Data Mining: Proceedings of the Third International Conference on Soft Computing and Data Mining (SCDM 2018), Johor, Malaysia, February 06-07, 2018},
	author={Ghazali, R. and Deris, M.M. and Nawi, N.M. and Abawajy, J.H.},
	series={Advances in Intelligent Systems and Computing},
	year={2018},
	publisher={Springer International Publishing}
}

@article{bennett2000support,
	title={Support vector machines: hype or hallelujah?},
	author={Bennett, Kristin P and Campbell, Colin},
	journal={Acm Sigkdd Explorations Newsletter},
	volume={2},
	number={2},
	pages={1--13},
	year={2000},
	publisher={Acm New York, NY, USA}
}

@ARTICLE{7079589,
	author={R. C. Guido},
	journal={IEEE Signal Processing Magazine}, 
	title={Practical and Useful Tips on Discrete Wavelet Transforms [sp Tips   Tricks]}, 
	year={2015},
	volume={32},
	number={3},
	pages={162-166},
	doi={10.1109/MSP.2014.2368586},
	month={May},
}

@misc{WAVE2019,
	author = {Craig Stuart Sapp},
	title = {WAVE PCM soundfile format},
	date = {2019-11-14},
	url = {http://soundfile.sapp.org/doc/WaveFormat/},
	year = {2019},
}


@article{adiga2007writing,
	title={Writing endian-independent code in C},
	author={Adiga, H},
	journal={Retrieved April},
	volume={24},
	year={2007},
}


@manual{microsoftIbmWaveSpec,
	organization = "IBM and Microsoft",
	title = "Multimedia Programming Interface and Data Specifications 1.0",
	year = 1991,
	note = "Rev. 1.0"
}

@Article{Ren2019,
	author={Ren, Yanzhen and Fang, Zhong and Liu, Dengkai and Chen, Changwen},
	title={Replay attack detection based on distortion by loudspeaker for voice authentication},
	journal={Multimedia Tools and Applications},
	year={2019},
	month={Apr},
	day={01},
	volume={78},
	number={7},
	pages={8383--8396},
	doi={10.1007/s11042-018-6834-3},
}


@ARTICLE{7802552,
	author={K. Sriskandaraja; V. Sethu; E. Ambikairajah; H. Li},
	journal={IEEE Journal of Selected Topics in Signal Processing},
	title={Front-End for Antispoofing Countermeasures in Speaker Verification: Scattering Spectral Decomposition},
	year={2017},
	volume={11},
	number={4},
	pages={632-643},
	doi={10.1109/JSTSP.2016.2647202},
	month={June},
}


@misc{SAS2015,
	author = {Wu, Zhizheng; et al.},
	title = {SAS 2015},
	date = {2015-09-17},
	year = {2015},
}

@misc{SAS2017,
	author = {Kinnunen, Tomi; et al.},
	title = {SAS 2017},
	date = {2017-08-18},
	url = {https://datashare.is.ed.ac.uk/handle/10283/2778},
	year = {2017},
}

@misc{SAS2019,
	author = {Yamagishi, Junichi; et al.},
	title = {SAS 2019},
	date = {2019-06-04},
	url = {https://datashare.is.ed.ac.uk/handle/10283/3336},
	year = {2019},
}

@misc{redDots,
	author = {RedDots},
	title = {RedDots database},
	year = {2015},
	url = {https://sites.google.com/site/thereddotsproject/home},
}

@misc{AVSpoof2015,
	organization = {Idiap Research Institute},
	title = {AVspoof Database 2015},
	url = {https://www.idiap.ch/dataset/avspoof},
	year = {2015},
}

@article{alluri2019replay,
	title={Replay spoofing countermeasures using high spectro-temporal resolution features},
	author={Alluri, KNRK Raju and Vuppala, Anil Kumar},
	journal={International Journal of Speech Technology},
	volume={22},
	number={1},
	pages={271--281},
	year={2019},
	publisher={Springer}
}

@INPROCEEDINGS{8725688,
	author={Y. {Ye} and L. {Lao} and D. {Yan} and L. {Lin}},
	title={Detection of Replay Attack Based on Normalized Constant Q Cepstral Feature},
	year={2019},
	volume={},
	number={},
	pages={407-411},
	doi={10.1109/ICCCBDA.2019.8725688},
	month={April},
}

@Article{Hanilci2018,
	author="Hanil{\c{c}}i, Cemal",
	title="Linear prediction residual features for automatic speaker verification anti-spoofing",
	journal="Multimedia Tools and Applications",
	year="2018",
	month="Jul",
	day="01",
	volume="77",
	number="13",
	pages="16099--16111",
	doi="10.1007/s11042-017-5181-0",
}

@inproceedings{ ISI:000473343500086,
	Author = {Rahmeni, Raoudha and Ben Aicha, Anis and Ben Ayed, Yassine},
	Book-Group-Author = {{IEEE}},
	Title = {{On the contribution of the voice texture for speech spoofing detection}},
	Year = {{2019}},
	Pages = {{501-505}},
	Publisher = {{IEEE}},
	Type = {{Proceedings Paper}},
}

@article{TODISCO2017516,
    author = {Massimiliano Todisco and Héctor Delgado and Nicholas Evans},
	title = "Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification",
	journal = "Computer Speech \& Language",
	volume = "45",
	pages = "516 - 535",
	year = "2017",
	doi = "https://doi.org/10.1016/j.csl.2017.01.001",
}

@inproceedings{Patel2015,
	author = {Patel, Tanvina and Patil, Hemant},
	year = {2015},
	month = {09},
	pages = {},
	title = {Combining Evidences from Mel Cepstral, Cochlear Filter Cepstral and Instantaneous Frequency Features for Detection of Natural vs. Spoofed Speech}
}

@INPROCEEDINGS{7472724,
	author={S. {Novoselov} and A. {Kozlov} and G. {Lavrentyeva} and K. {Simonchik} and V. {Shchemelinin}},
	title={STC anti-spoofing systems for the ASVspoof 2015 challenge},
	year={2016},
	volume={},
	number={},
	pages={5475-5479},
	doi={10.1109/ICASSP.2016.7472724},
	month={March},
}

@inproceedings{korshunov2016overview,
	title={Overview of BTAS 2016 speaker anti-spoofing competition},
	author={Korshunov, Pavel and Marcel, S{\'e}bastien and Muckenhirn, Hannah and Gon{\c{c}}alves, Andr{\'e} R and Mello, AG Souza and Violato, RP Velloso and Simoes, Fl{\'a}vio O and Neto, M Uliani and de Assis Angeloni, Marcus and Stuchi, Jos{\'e} Augusto and others},
	pages={1--6},
	year={2016},
	organization={IEEE}
}

@inproceedings{ ISI:000490497200068,
	Author = {Saranya, M. S. and Padmanabhan, R. and Murthy, Hema A.},
	Book-Group-Author = {{IEEE}},
	Title = {{Replay Attack Detection in Speaker Verification Using non-voiced
	segments and Decision Level Feature Switching}},
	Series = {{International Conference on Signal Processing and Communications SPCOM}},
	Year = {{2018}},
	Pages = {{332-336}},
	Organization = {{IEEE}},
}

@inproceedings{ ISI:000465363900136,
	Author = {Kamble, Madhu R. and Patil, Hemant A.},
	Book-Group-Author = {{Int Speech Commun Assoc}},
	Title = {{Novel Variable Length Energy Separation Algorithm using Instantaneous	Amplitude Features For Replay Detection}},
	Series = {{Interspeech}},
	Year = {{2018}},
	Pages = {{646-650}},
	Organization = {{Int Speech Commun Assoc}},
	DOI = {{10.21437/Interspeech.2018-1687}},
}

@inproceedings{ ISI:000465363900139,
	Author = {Wickramasinghe, Buddhi and Irtza, Saad and Ambikairajah, Eliathamby and
	Epps, Julien},
	Book-Group-Author = {{Int Speech Commun Assoc}},
	Title = {{Frequency Domain Linear Prediction Features for Replay Spoofing Attack
	Detection}},
	Series = {{Interspeech}},
	Year = {{2018}},
	Pages = {{661-665}},
	Organization = {{Int Speech Commun Assoc}},
	Publisher = {{ISCA-INT SPEECH COMMUNICATION ASSOC}},
	Type = {{Proceedings Paper}},
	DOI = {{10.21437/Interspeech.2018-1574}},
}

@inproceedings{Suthokumar2018,
	author={Gajan Suthokumar and Vidhyasaharan Sethu and Chamith Wijenayake and Eliathamby Ambikairajah},
	title={Modulation Dynamic Features for the Detection of Replay Attacks},
	year=2018,
	pages={691--695},
	doi={10.21437/Interspeech.2018-1846},
}

@inproceedings{ ISI:000458728700054,
	Author = {Kamble, Madhu R. and Patil, Hemant A.},
	Book-Group-Author = {{IEEE}},
	Title = {{Novel Energy Separation Based Frequency Modulation Features For Spoofed Speech Classification}},
	Year = {{2017}},
	Pages = {{326-331}},
}

@inproceedings{ISI:000392503100008,
	Author = {Das, K. Arun and George, Kuruvachan K. and Kumar, C. Santhosh and Veni, S. and Panda, Ashish},
	Title = {{Modified Gammatone Frequency Cepstral Coefficients to Improve Spoofing Detection}},
	Year = {{2016}},
	Pages = {{50-55}},
	Publisher = {{IEEE}},
	Type = {{Proceedings Paper}},
}

@Article{DiqunYan2019,
	author="Diqun Yan and Xiang, Li	and Wang, Zhifeng  and Wang, Rangding",
	title="Detection of HMM Synthesized Speech by Wavelet Logarithmic Spectrum",
	journal="Automatic Control and Computer Sciences",
	year="2019",
	month="Jan",
	day="01",
	volume="53",
	number="1",
	pages="72--79",
	doi="10.3103/S014641161901005X",
}

@article{johansson1999hilbert,
	title={The hilbert transform},
	author={Johansson, Mathias},
	journal={Mathematics Master’s Thesis. V{\"a}xj{\"o} University, Suecia. Disponible en internet: http://w3. msi. vxu. se/exarb/mj\_ex. pdf, consultado el},
	volume={19},
	year={1999}
}

@article{kschischang2006hilbert,
	title={The hilbert transform},
	author={Kschischang, Frank R},
	journal={University of Toronto},
	volume={83},
	pages={277},
	year={2006},
	publisher={Citeseer}
}

@online{WaveletPropertiesBrowser,
	author = {Filip Wasilewski},
	title = {Wavelet Properties Browser},
	date = {2020-07-06},
	url = {http://wavelets.pybytes.com/wavelet/haar/},
	subtitle = {Wavelet Haar (haar)},
	year = {2020},
}@misc{2023Gibbs,
	note = {[Online; accessed 2023-11-19]},
	year = {2023},
	month = {jul 7},
	publisher = {},
	title = {Gibbs {Energy} and {Redox} {Reactions}},
}


@book{gerstner2014neuronal,
	title={Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
	author={Gerstner, W. and Kistler, W.M. and Naud, R. and Paninski, L.},
	isbn={9781107060838},
	lccn={2013047693},
	url={https://neuronaldynamics.epfl.ch/online/Ch2.S2.html},
	year={2014},
	publisher={Cambridge University Press},
	chapter={2}
}

@book{sanei2021eeg,
	title={EEG signal processing and machine learning},
	author={Sanei, Saeid and Chambers, Jonathon A},
	year={2021},
	publisher={John Wiley \& Sons}
}


@article{JALALYBIDGOLY2020101788,
	title = {A survey on methods and challenges in EEG based authentication},
	journal = {Computers and Security},
	volume = {93},
	pages = {101788},
	year = {2020},
	issn = {0167-4048},
	doi = {https://doi.org/10.1016/j.cose.2020.101788},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404820300730},
	author = {A. J. Bidgoly and H. J. Bidgoly and Z. Arezoumand},
	keywords = {EEG, Biometric factor, Authentication, User identification, Pattern recognition, Survey},
}


@misc{hasani2020liquid,
	title={Liquid Time-constant Networks}, 
	author={Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
	year={2020},
	eprint={2006.04439},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@InProceedings{10.1007/978-3-540-39432-7_63,
	author="Fernando, Chrisantha
	and Sojakka, Sampsa",
	editor="Banzhaf, Wolfgang
	and Ziegler, Jens
	and Christaller, Thomas
	and Dittrich, Peter
	and Kim, Jan T.",
	title="Pattern Recognition in a Bucket",
	booktitle="Advances in Artificial Life",
	year="2003",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="588--597",
	abstract="This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.",
	isbn="978-3-540-39432-7"
}


@book{kasabov2019time,
	title={Time-space, spiking neural networks and brain-inspired artificial intelligence},
	author={Kasabov, Nikola K},
	year={2019},
	publisher={Springer}
}

@misc{dan_goodman_2022_7044500,
	author       = {Dan Goodman and
	Tomas Fiers and
	Richard Gao and
	Marcus Ghosh and
	Nicolas Perez},
	title        = {{Spiking Neural Network Models in Neuroscience - 
	Cosyne Tutorial 2022}},
	month        = sep,
	year         = 2022,
	publisher    = {Zenodo},
	version      = {1.0},
	doi          = {10.5281/zenodo.7044500},
	url          = {https://doi.org/10.5281/zenodo.7044500}
}

@misc{jones2020single,
	title={Can Single Neurons Solve MNIST? The Computational Power of Biological Dendritic Trees},
	author={Ilenna Simone Jones and Konrad Paul Kording},
	year={2020},
	eprint={2009.01269},
	archivePrefix={arXiv},
	primaryClass={q-bio.NC},
	url={https://arxiv.org/abs/2009.01269}
}

@ARTICLE{6789852,
	author={Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	journal={Neural Computation}, 
	title={Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}, 
	year={2002},
	volume={14},
	number={11},
	pages={2531-2560},
	doi={10.1162/089976602760407955}}


@inbook{doi:10.1142/9781848162778_0008,
	author = { Wolfgang   Maass },
	title = {Liquid State Machines: Motivation, Theory, and Applications},
	booktitle = {Computability in Context},
	chapter = {},
	pages = {275-296},
	doi = {10.1142/9781848162778_0008},
	URL = {https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008},
	eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9781848162778_0008},
	abstract = { Abstract The Liquid State Machine (LSM) has emerged as a computational model that is more adequate than the Turing machine for describing computations in biological networks of neurons. Characteristic features of this new model are (i) that it is a model for adaptive computational systems, (ii) that it provides a method for employing randomly connected circuits, or even “found” physical objects for meaningful computations, (iii) that it provides a theoretical context where heterogeneous, rather than stereo typical, local gates, or processors increase the computational power of a circuit, (iv) that it provides a method for multiplexing different computations (on a common input) within the same circuit. This chapter reviews the motivation for this model, its theoretical background, and current work on implementations of this model in innovative artificial computing devices. }
}

@article{Hasani2022,
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Ray, Aaron and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},
	title = {Closed-form continuous-time neural networks},
	journal = {Nature Machine Intelligence},
	volume = {4},
	number = {11},
	pages = {992-1003},
	year = {2022},
	month = {November},
	doi = {10.1038/s42256-022-00556-7},
	url = {https://doi.org/10.1038/s42256-022-00556-7},
	issn = {2522-5839},
	date = {2022-11-01},
}
@online{liquid_nn_article,
	author = {Bhaumik Tyagi},
	title = {Liquid Neural Networks: Revolutionizing AI with Dynamic Information Flow},
	year = {2023},
	url = {https://tyagi-bhaumik.medium.com/liquid-neural-networks-revolutionizing-ai-with-dynamic-information-flow-30e27f1cc912},
	note = {Medium article},
}

@ARTICLE{10242251,
	author={Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
	journal={Proceedings of the IEEE}, 
	title={Training Spiking Neural Networks Using Lessons From Deep Learning}, 
	year={2023},
	volume={111},
	number={9},
	pages={1016-1054},
	doi={10.1109/JPROC.2023.3308088}
}
@MastersThesis{PRG16,
	author       = "Pressel-Coreto, G. and Rufiner, H. L. and Gareis, I. E.",
	title        = "Diseño y elaboración de una base de datos pública de 
	registros electroencefalográficos orientados a la 
	clasificación de habla imaginada. (Undergraduate project)",
	school       = "Universidad Nacional de Entre Ríos - Facultad de Ingeniería",
	year         = "2016",
	url          = "http://sinc.unl.edu.ar/sinc-publications/2016/PRG16"
}

@article{ScienceOpenVid:5960cfa8-7fde-441c-8592-35fdb9841499,
	author = {G H Klem, H O Lüders and H H Jasper et al. },
	title = {The ten-twenty electrode system of the International Federation. The International Federation of Clinical Neurophysiology.},
	journal = {Electroencephalography and clinical neurophysiology. Supplement},
	year = 1999,
	volume = {52},
	number = {},
	pages = {},
	doi = {}
}

@misc{sistema10-20,
	author = {Edrin Vicente},
	title = {Sistema 10-20 para localizar alvos terapêuticos em EMT},
	date = {2023/05/11 04:31},
	url = {https://www.kandel.com.br/post/como-localizar-os-pontos-de-estimulacao-para-estimulacao-magnetica-transcraniana},
	year = {2023}
}


@article{AbdulghaniMokhlesM2023ISCU,
	number = {6},
	pages = {649-},
	publisher = {MDPI AG},
	title = {Imagined Speech Classification Using EEG and Deep Learning},
	volume = {10},
	year = {2023},
	abstract = {In this paper, we propose an imagined speech-based brain wave pattern recognition using deep learning. Multiple features were extracted concurrently from eight-channel electroencephalography (EEG) signals. To obtain classifiable EEG data with fewer sensors, we placed the EEG sensors on carefully selected spots on the scalp. To decrease the dimensions and complexity of the EEG dataset and to avoid overfitting during the deep learning algorithm, we utilized the wavelet scattering transformation. A low-cost 8-channel EEG headset was used with MATLAB 2023a to acquire the EEG data. The long-short term memory recurrent neural network (LSTM-RNN) was used to decode the identified EEG signals into four audio commands: up, down, left, and right. Wavelet scattering transformation was applied to extract the most stable features by passing the EEG dataset through a series of filtration processes. Filtration was implemented for each individual command in the EEG datasets. The proposed imagined speech-based brain wave pattern recognition approach achieved a 92.50% overall classification accuracy. This accuracy is promising for designing a trustworthy imagined speech-based brain-computer interface (BCI) future real-time systems. For better evaluation of the classification performance, other metrics were considered, and we obtained 92.74%, 92.50%, and 92.62% for precision, recall, and F1-score, respectively.},
	author = {Abdulghani, Mokhles M and Walters, Wilbur L and Abed, Khalid H},
	address = {Switzerland},
	copyright = {COPYRIGHT 2023 MDPI AG},
	issn = {2306-5354},
	journal = {Bioengineering (Basel)},
	keywords = {Accuracy ; Algorithms ; Bioengineering ; Brain ; Brain research ; brain–computer interface (BCI) ; Classification ; Computer applications ; Datasets ; Deep learning ; Design ; EEG ; EEG decoding ; Electroencephalography ; Filtration ; Human-computer interface ; imagined speech ; Implants ; inner speech ; Internet of Things ; LSTM ; Machine learning ; Methods ; Neural networks ; Pattern recognition ; Recurrent neural networks ; Research methodology ; Scattering ; Sensors ; Short term memory ; Signal classification ; Speech ; Systems design ; Voice recognition ; wavelet scattering transformation (WST)},
	language = {eng},
}


@article{MahapatraNrushinghCharan2023Ecoi,
	number = {2},
	pages = {26040-},
	publisher = {IOP Publishing},
	title = {EEG-based classification of imagined digits using a recurrent neural network},
	volume = {20},
	year = {2023},
	abstract = {In recent years, imagined speech brain-computer (machine) interface applications have been an important field of study that can improve the lives of patients with speech problems through alternative verbal communication. This study aims to classify the imagined speech of numerical digits from electroencephalography (EEG) signals by exploiting the past and future temporal characteristics of the signal using several deep learning models.
	This study proposes a methodological combination of EEG signal processing techniques and deep learning models for the recognition of imagined speech signals. EEG signals were filtered and preprocessed using the discrete wavelet transform to remove artifacts and retrieve feature information. To classify the preprocessed imagined speech neural signals, multiple versions of multilayer bidirectional recurrent neural networks were used.
	The method is examined by leveraging MUSE and EPOC signals from MNIST imagined digits in the MindBigData open-access database. The presented methodology's classification performance accuracy was noteworthy, with the model's multiclass overall classification accuracy reaching a maximum of 96.18% on MUSE signals and 71.60% on EPOC signals.
	This study shows that the proposed signal preprocessing approach and the stacked bidirectional recurrent network model are suitable for extracting the high temporal resolution of EEG signals in order to classify imagined digits, indicating the unique neural identity of each imagined digit class that distinguishes it from the others.},
	author = {Mahapatra, Nrushingh Charan and Bhuyan, Prachet},
	address = {England},
	copyright = {2023 IOP Publishing Ltd},
	issn = {1741-2560},
	journal = {Journal of neural engineering},
	keywords = {Algorithms ; Alprostadil ; bidirectional recurrent neural network ; Brain-Computer Interfaces ; deep learning ; electroencephalography (EEG) ; Electroencephalography - methods ; Humans ; imagined speech ; Neural Networks, Computer ; signal processing},
	language = {eng},
}


@online{mindbigdata,
	author = {David Vivancos},
	title = {MindBigData},
	date = {December 30th 2023},
	url = {https://mindbigdata.com/opendb/},
	language = {english},
	month = {December},
	year = {2023},
	accessed = {2024-05-01}
}


@article{ShahUzair2022TRoA,
	number = {18},
	pages = {6975-},
	publisher = {MDPI AG},
	title = {The Role of Artificial Intelligence in Decoding Speech from EEG Signals: A Scoping Review},
	volume = {22},
	year = {2022},
	abstract = {Background: Brain traumas, mental disorders, and vocal abuse can result in permanent or temporary speech impairment, significantly impairing one’s quality of life and occasionally resulting in social isolation. Brain–computer interfaces (BCI) can support people who have issues with their speech or who have been paralyzed to communicate with their surroundings via brain signals. Therefore, EEG signal-based BCI has received significant attention in the last two decades for multiple reasons: (i) clinical research has capitulated detailed knowledge of EEG signals, (ii) inexpensive EEG devices, and (iii) its application in medical and social fields. Objective: This study explores the existing literature and summarizes EEG data acquisition, feature extraction, and artificial intelligence (AI) techniques for decoding speech from brain signals. Method: We followed the PRISMA-ScR guidelines to conduct this scoping review. We searched six electronic databases: PubMed, IEEE Xplore, the ACM Digital Library, Scopus, arXiv, and Google Scholar. We carefully selected search terms based on target intervention (i.e., imagined speech and AI) and target data (EEG signals), and some of the search terms were derived from previous reviews. The study selection process was carried out in three phases: study identification, study selection, and data extraction. Two reviewers independently carried out study selection and data extraction. A narrative approach was adopted to synthesize the extracted data. Results: A total of 263 studies were evaluated; however, 34 met the eligibility criteria for inclusion in this review. We found 64-electrode EEG signal devices to be the most widely used in the included studies. The most common signal normalization and feature extractions in the included studies were the bandpass filter and wavelet-based feature extraction. We categorized the studies based on AI techniques, such as machine learning and deep learning. The most prominent ML algorithm was a support vector machine, and the DL algorithm was a convolutional neural network. Conclusions: EEG signal-based BCI is a viable technology that can enable people with severe or temporal voice impairment to communicate to the world directly from their brain. However, the development of BCI technology is still in its infancy.},
	author = {Shah, Uzair and Alzubaidi, Mahmood and Mohsen, Farida and Abd-Alrazaq, Alaa and Alam, Tanvir and Househ, Mowafa},
	address = {Basel},
	copyright = {COPYRIGHT 2022 MDPI AG},
	issn = {1424-8220},
	journal = {Sensors (Basel, Switzerland)},
	keywords = {Algorithms ; Artificial intelligence ; Bandpass filters ; Communication ; Data acquisition ; Deep learning ; Digital systems ; Discriminant analysis ; electroencephalogram ; Electroencephalography ; Electromyography ; Feature extraction ; Human-computer interface ; imagine speech ; Impairment ; Machine learning ; Medical research ; Medicine, Experimental ; Mental disorders ; Neural networks ; Review ; sensors ; signals ; Speech ; speech decoding ; Support vector machines ; Voice communication},
	language = {eng},
}

@article{MahapatraNrushinghCharan2022MCoI,
	pages = {1-10},
	publisher = {Hindawi},
	title = {Multiclass Classification of Imagined Speech Vowels and Words of Electroencephalography Signals Using Deep Learning},
	volume = {2022},
	year = {2022},
	abstract = {The paper’s emphasis is on the imagined speech decoding of electroencephalography (EEG) neural signals of individuals in accordance with the expansion of the brain-computer interface to encompass individuals with speech problems encountering communication challenges. Decoding an individual’s imagined speech from nonstationary and nonlinear EEG neural signals is a complex task. Related research work in the field of imagined speech has revealed that imagined speech decoding performance and accuracy require attention to further improve. The evolution of deep learning technology increases the likelihood of decoding imagined speech from EEG signals with enhanced performance. We proposed a novel supervised deep learning model that combined the temporal convolutional networks and the convolutional neural networks with the intent of retrieving information from the EEG signals. The experiment was carried out using an open-access dataset of fifteen subjects’ imagined speech multichannel signals of vowels and words. The raw multichannel EEG signals of multiple subjects were processed using discrete wavelet transformation technique. The model was trained and evaluated using the preprocessed signals, and the model hyperparameters were adjusted to achieve higher accuracy in the classification of imagined speech. The experiment results demonstrated that the multiclass imagined speech classification of the proposed model exhibited a higher overall accuracy of 0.9649 and a classification error rate of 0.0350. The results of the study indicate that individuals with speech difficulties might well be able to leverage a noninvasive EEG-based imagined speech brain-computer interface system as one of the long-term alternative artificial verbal communication mediums.},
	author = {Mahapatra, Nrushingh Charan and Bhuyan, Prachet},
	address = {New York},
	copyright = {Copyright © 2022 Nrushingh Charan Mahapatra and Prachet Bhuyan.},
	issn = {1687-5893},
	journal = {Advances in human-computer interaction},
	keywords = {Accuracy ; Analysis ; Artificial neural networks ; Brain ; Brain research ; Classification ; Communication ; Computer applications ; Deep learning ; Discrete Wavelet Transform ; Discriminant analysis ; EEG ; Electroencephalography ; Eye movements ; Human-computer interface ; Implants ; Information retrieval ; Machine learning ; Multichannel communication ; Muscle function ; Neural networks ; Performance enhancement ; Signal processing ; Speech ; Support vector machines ; Verbal communication ; Vowels ; Words (language)},
	language = {eng},
}

@article{AgarwalPrabhakar2022Ebia,
	abstract = {Imagined speech is a neuro‐paradigm that can provide an alternative communication channel for patients in a locked‐in syndrome state. We have performed an experiment in which a 32 channel industry‐standard electroencephalography (EEG) device was used to record 26 imagined English alphabets from 13 subjects. We denoised the imagined signals by discrete wavelet transform and extracted the spatial filters by common spatial pattern method, and time‐domain features. Spatial features when classified with linear support vector machine, and time‐domain features classified by random forest gave the best results. Alpha, beta, and theta bands could classify imagined alphabets better than other bands and had average classification accuracies of 88.59%, 87.39%, and 88.97%, respectively by using spatial features and 81.88%, 76.72%, and 79.25%, respectively, by time‐domain features. The grand average accuracies of all the 26 alphabets in six EEG frequency bands was found to be 77.97% in a subject independent binary classification framework.},
	author = {Agarwal, Prabhakar and Kumar, Sandeep},
	address = {Hoboken, USA},
	copyright = {2021 Wiley Periodicals LLC.},
	issn = {0899-9457},
	journal = {International journal of imaging systems and technology},
	keywords = {brain–computer interface ; Classification ; Discrete Wavelet Transform ; Domains ; electroencephalogram ; Electroencephalography ; Electromagnetic wave filters ; Frequencies ; imagined alphabets ; Pattern method (forecasting) ; Spatial filtering ; Support vector machines ; Wavelet transforms},
	language = {eng},
	number = {1},
	pages = {111-122},
	publisher = {John Wiley and Sons, Inc},
	title = {Electroencephalography based imagined alphabets classification using spatial and time‐domain features},
	volume = {32},
	year = {2022},
}

@article{Hernandez-Del-ToroTonatiuh2021TaEB,
	abstract = {An asynchronous Brain--Computer Interface (BCI) based on imagined speech is a tool that allows to control an external device or to emit a message at the moment the user desires to by decoding EEG signals of imagined speech. In order to correctly implement these types of BCI, we must be able to detect from a continuous signal, when the subject starts to imagine words. In this work, five methods of feature extraction based on wavelet decomposition, empirical mode decomposition, frequency energies, fractal dimension and chaos theory features are presented to solve the task of detecting imagined words segments from continuous EEG signals as a preliminary study for a latter implementation of an asynchronous BCI based on imagined speech. These methods are tested in three datasets using four different classifiers and the higher F1 scores obtained are 0.73, 0.79, and 0.68 for each dataset, respectively. This results are promising to build a system that automatizes the segmentation of imagined words segments for latter classification.},
	author = {Hernández-Del-Toro, Tonatiuh and Reyes-García, Carlos A and Villaseñor-Pineda, Luis},
	address = {Ithaca},
	copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
	issn = {2331-8422},
	journal = {arXiv.org},
	keywords = {Chaos theory ; Computer Science - Human-Computer Interaction ; Computer Science - Learning ; Datasets ; Electroencephalography ; Feature extraction ; Fractal geometry ; Human-computer interface ; Segmentation ; Segments ; Speech},
	language = {eng},
	publisher = {Cornell University Library, arXiv.org},
	title = {Toward asynchronous EEG-based BCI: Detecting imagined words segments in continuous EEG signals},
	year = {2021},
}


@article{MOCTEZUMA2019201,
	title = {Subjects identification using EEG-recorded imagined speech},
	journal = {Expert Systems with Applications},
	volume = {118},
	pages = {201-208},
	year = {2019},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2018.10.004},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306468},
	author = {Luis Alfredo Moctezuma and Alejandro A. Torres-García and Luis Villaseñor-Pineda and Maya Carrillo},
	keywords = {Subject identification, Electroencephalograms (EEG), Imagined speech, Biometrics},
	abstract = {Due to the problems presented in current traditional/biometric security systems, the interest to use new security systems, have been increasing. This paper explores the use of brain signals EEG-based during imagined speech in order to use it as a new biometric measure for Subjects identification and thus create a new biometric security system. The main contribution of this paper are two methods for feature extraction, first to improve the signal-to-noise ratio the Common Average Reference was applied. The first method was based on Discrete Wavelet Transform, and the second method was based on statistical features directly from the raw signal. The proposed methods were tested in a dataset of 27 Subjects who performed 33 repetitions of 5 imagined words in Spanish. The results show the feasibility of the task with accurate identification of the Subject, regardless of the imagined word used and using a commercial EEG system (EMOTIV EPOC). In addition, the scope of the method is displayed by decreasing the training data, as well as the number of active sensors for the identification task. Using the proposed method with future improvements and implementing it in a low-cost device can be a new and valuable biometric security system.}
}

@article{PanachakelJerrinRamakrishnan,
	author = {Panachakel, Jerrin and Ramakrishnan, A.G.},
	year = {2021},
	month = {10},
	pages = {},
	title = {Decoding Imagined Speech from EEG Using Transfer Learning},
	volume = {PP},
	journal = {IEEE Access},
	doi = {10.1109/ACCESS.2021.3116196}
}

@article{nguyen2018inferring,
	title={Inferring imagined speech using EEG signals: a new approach using Riemannian manifold features},
	author={Nguyen, Cuong Hoang and Karavas, Georgios Konstantinos and Artemiadis, Panagiotis},
	journal={Journal of neural engineering},
	volume={15},
	number={1},
	pages={016002},
	year={2018},
	doi={10.1088/1741-2552/aa8235},
	url={https://doi.org/10.1088/1741-2552/aa8235}
}

@article{TammMarkus-Oliver2020CoVf,
	abstract = {Imagined speech is a relatively new electroencephalography (EEG) neuro-paradigm, which has seen little use in Brain-Computer Interface (BCI) applications. Imagined speech can be used to allow physically impaired patients to communicate and to use smart devices by imagining desired commands and then detecting and executing those commands in a smart device. The goal of this research is to verify previous classification attempts made and then design a new, more efficient neural network that is noticeably less complex (fewer number of layers) that still achieves a comparable classification accuracy. The classifiers are designed to distinguish between EEG signal patterns corresponding to imagined speech of different vowels and words. This research uses a dataset that consists of 15 subjects imagining saying the five main vowels (a, e, i, o, u) and six different words. Two previous studies on imagined speech classifications are verified as those studies used the same dataset used here. The replicated results are compared. The main goal of this study is to take the proposed convolutional neural network (CNN) model from one of the replicated studies and make it much more simpler and less complex, while attempting to retain a similar accuracy. The pre-processing of data is described and a new CNN classifier with three different transfer learning methods is described and used to classify EEG signals. Classification accuracy is used as the performance metric. The new proposed CNN, which uses half as many layers and less complex pre-processing methods, achieved a considerably lower accuracy, but still managed to outperform the initial model proposed by the authors of the dataset by a considerable margin. It is recommended that further studies investigating classifying imagined speech should use more data and more powerful machine learning techniques. Transfer learning proved beneficial and should be used to improve the effectiveness of neural networks.},
	author = {Tamm, Markus-Oliver and Muhammad, Yar and Muhammad, Naveed},
	address = {Basel},
	copyright = {COPYRIGHT 2020 MDPI AG},
	issn = {2073-431X},
	journal = {Computers (Basel)},
	keywords = {Accuracy ; Applied research ; Artificial neural networks ; Classification ; Classifiers ; convolutional neural networks ; Datasets ; Deep learning ; Discriminant analysis ; EEG ; Electroencephalography ; Electronic devices ; Human-computer interface ; imagined speech ; Machine learning ; Methods ; Neural networks ; Signal classification ; Speech ; Studies ; Support vector machines ; transfer learning ; Vowels ; Wavelet transforms ; Words (language)},
	language = {eng},
	number = {2},
	pages = {46-},
	publisher = {MDPI AG},
	title = {Classification of Vowels from Imagined Speech with Convolutional Neural Networks},
	volume = {9},
	year = {2020},
}

@article{dasalla2009single,
	title={Single-trial classification of vowel speech imagery using common spatial patterns},
	author={DaSalla, C. S. and Kambara, H. and Sato, M. and Koike, Y.},
	journal={Neural networks : the official journal of the International Neural Network Society},
	volume={22},
	number={9},
	pages={1334--1339},
	year={2009},
	publisher={Elsevier},
	doi={10.1016/j.neunet.2009.05.008}
}


@Article{math11194205,
	AUTHOR = {Wang, Shuqi and Zhang, Huajun and Zhang, Xuetao and Su, Yixin and Wang, Zhenghua},
	TITLE = {Voiceprint Recognition under Cross-Scenario Conditions Using Perceptual Wavelet Packet Entropy-Guided Efficient-Channel-Attention–Res2Net–Time-Delay-Neural-Network Model},
	JOURNAL = {Mathematics},
	VOLUME = {11},
	YEAR = {2023},
	NUMBER = {19},
	ARTICLE-NUMBER = {4205},
	URL = {https://www.mdpi.com/2227-7390/11/19/4205},
	ISSN = {2227-7390},
	ABSTRACT = {(1) Background: Voiceprint recognition technology uses individual vocal characteristics for identity authentication and faces many challenges in cross-scenario applications. The sound environment, device characteristics, and recording conditions in different scenarios cause changes in sound features, which, in turn, affect the accuracy of voiceprint recognition. (2) Methods: Based on the latest trends in deep learning, this paper uses the perceptual wavelet packet entropy (PWPE) method to extract the basic voiceprint features of the speaker before using the efficient channel attention (ECA) block and the Res2Net block to extract deep features. The PWPE block removes the effect of environmental noise on voiceprint features, so the perceptual wavelet packet entropy-guided ECA–Res2Net–Time-Delay-Neural-Network (PWPE-ECA-Res2Net-TDNN) model shows an excellent robustness. The ECA-Res2Net-TDNN block uses temporal statistical pooling with a multi-head attention mechanism to weight frame-level audio features, resulting in a weighted average of the final representation of the speech-level feature vectors. The sub-center ArcFace loss function is used to enhance intra-class compactness and inter-class differences, avoiding classification via output value alone like the softmax loss function. Based on the aforementioned elements, the PWPE-ECA-Res2Net-TDNN model for speaker recognition is designed to extract speaker feature embeddings more efficiently in cross-scenario applications. (3) Conclusions: The experimental results demonstrate that, compared to the ECAPA-TDNN model using MFCC features, the PWPE-based ECAPA-TDNN model performs better in terms of cross-scene recognition accuracy, exhibiting a stronger robustness and better noise resistance. Furthermore, the model maintains a relatively short recognition time even under the highest recognition rate conditions. Finally, a set of ablation experiments targeting each module of the proposed model is conducted. The results indicate that each module contributes to an improvement in the recognition performance.},
	DOI = {10.3390/math11194205}
}

@article{gao2019res2net,
	title={Res2Net: A New Multi-scale Backbone Architecture},
	author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},
	journal={IEEE TPAMI},
	year={2021},
	doi={10.1109/TPAMI.2019.2938758}, 
}

@article{DBLP:journals/corr/abs-1910-03151,
	author       = {Qilong Wang and
	Banggu Wu and
	Pengfei Zhu and
	Peihua Li and
	Wangmeng Zuo and
	Qinghua Hu},
	title        = {ECA-Net: Efficient Channel Attention for Deep Convolutional Neural
	Networks},
	journal      = {CoRR},
	volume       = {abs/1910.03151},
	year         = {2019},
	url          = {http://arxiv.org/abs/1910.03151},
	eprinttype    = {arXiv},
	eprint       = {1910.03151},
	timestamp    = {Mon, 04 Dec 2023 21:30:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-03151.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fan2020cn,
	title={CN-CELEB: a challenging Chinese speaker recognition dataset},
	author={Fan, Yue and Kang, JW and Li, LT and Li, KC and Chen, HL and Cheng, ST and Zhang, PY and Zhou, ZY and Cai, YQ and Wang, Dong},
	booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={7604--7608},
	year={2020},
	organization={IEEE}
}